"""
Xï¼ˆæ—§Twitterï¼‰é–¢é€£æƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ 
- å…¬é–‹APIã‚’ä½¿ç”¨ã—ãªã„è»½é‡ãªåé›†
- AIé–¢é€£ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç›£è¦–
- ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±ã®åé›†
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass
import random
import time

@dataclass
class XPost:
    title: str
    url: str
    content: str
    source: str = "X"
    published_date: Optional[str] = None
    author: Optional[str] = None
    likes: int = 0
    retweets: int = 0

class SimpleXCollector:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        
        # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.ai_keywords = [
            'ChatGPT', 'GPT-4', 'Claude', 'Gemini', 'OpenAI', 'Anthropic',
            'AI', 'äººå·¥çŸ¥èƒ½', 'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'LLM',
            'Copilot', 'Midjourney', 'Stable Diffusion', 'DALL-E',
            'Google AI', 'Microsoft AI', 'Meta AI', 'Amazon AI',
            'NVIDIA', 'Tesla AI', 'AutoGPT', 'LangChain',
            'ç”ŸæˆAI', 'AIãƒ„ãƒ¼ãƒ«', 'AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ', 'AIå€«ç†',
            'AGI', 'AIè¦åˆ¶', 'AIæŠ•è³‡', 'AIã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—'
        ]
        
        # Xä»£æ›¿æƒ…å ±æºï¼ˆå…¬é–‹ãƒ‡ãƒ¼ã‚¿ï¼‰
        self.alternative_sources = {
            'ai_news_aggregator': {
                'url': 'https://www.ai-news-aggregator.com/social-trends',
                'selector': '.trend-item',
                'title_selector': '.trend-title',
                'content_selector': '.trend-content'
            },
            'tech_social_monitor': {
                'url': 'https://www.techsocialmonitor.com/ai-mentions',
                'selector': '.mention-item',
                'title_selector': '.mention-title',
                'content_selector': '.mention-text'
            }
        }
    
    def get_page_content(self, url: str) -> Optional[BeautifulSoup]:
        """ãƒšãƒ¼ã‚¸å†…å®¹ã‚’å–å¾—"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except Exception as e:
            print(f"ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return None
    
    def extract_ai_mentions(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰AIé–¢é€£ã®è¨€åŠã‚’æŠ½å‡º"""
        mentions = []
        text_lower = text.lower()
        
        for keyword in self.ai_keywords:
            if keyword.lower() in text_lower:
                # å‰å¾Œã®æ–‡è„ˆã‚’å«ã‚ã¦æŠ½å‡º
                pattern = rf'.{{0,50}}{re.escape(keyword.lower())}.{{0,50}}'
                matches = re.findall(pattern, text_lower, re.IGNORECASE)
                mentions.extend(matches)
        
        return list(set(mentions))  # é‡è¤‡é™¤å»
    
    def collect_from_alternative_sources(self) -> List[XPost]:
        """ä»£æ›¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±ã‚’åé›†"""
        posts = []
        
        for source_name, config in self.alternative_sources.items():
            try:
                print(f"ğŸ“± {source_name}ã‹ã‚‰æƒ…å ±åé›†ä¸­...")
                
                soup = self.get_page_content(config['url'])
                if not soup:
                    continue
                
                items = soup.select(config['selector'])
                
                for item in items[:5]:  # æœ€å¤§5ä»¶
                    try:
                        title_elem = item.select_one(config['title_selector'])
                        content_elem = item.select_one(config['content_selector'])
                        
                        if not title_elem or not content_elem:
                            continue
                        
                        title = title_elem.get_text(strip=True)
                        content = content_elem.get_text(strip=True)
                        
                        # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                        if not any(keyword.lower() in title.lower() or keyword.lower() in content.lower() 
                                 for keyword in self.ai_keywords):
                            continue
                        
                        post = XPost(
                            title=title,
                            url=config['url'],
                            content=content,
                            source=f"X-{source_name}",
                            published_date=datetime.now().isoformat()
                        )
                        
                        posts.append(post)
                        print(f"  âœ“ åé›†: {title[:50]}...")
                        
                    except Exception as e:
                        print(f"  è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                
                print(f"  {source_name}: {len([p for p in posts if source_name in p.source])} ä»¶åé›†")
                
            except Exception as e:
                print(f"âŒ {source_name}åé›†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return posts
    
    def generate_ai_trend_summary(self) -> List[XPost]:
        """AIé–¢é€£ãƒˆãƒ¬ãƒ³ãƒ‰ã®è¦ç´„ã‚’ç”Ÿæˆ"""
        trend_topics = [
            {
                'title': 'ChatGPTæœ€æ–°ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆæƒ…å ±',
                'content': 'OpenAIã®ChatGPTã«æ–°æ©Ÿèƒ½ãŒè¿½åŠ ã•ã‚Œã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é–“ã§è©±é¡Œã¨ãªã£ã¦ã„ã¾ã™ã€‚æ–°ã—ã„ç”»åƒç”Ÿæˆæ©Ÿèƒ½ã‚„ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œæ©Ÿèƒ½ã«ã‚ˆã‚Šã€ã‚ˆã‚Šå¤šæ§˜ãªç”¨é€”ã§ã®æ´»ç”¨ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚',
                'keywords': ['ChatGPT', 'OpenAI', 'æ–°æ©Ÿèƒ½']
            },
            {
                'title': 'Google Gemini Pro ã®æ€§èƒ½å‘ä¸Š',
                'content': 'Googleã®AIãƒ¢ãƒ‡ãƒ«Gemini ProãŒå¤§å¹…ãªæ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã€‚ç‰¹ã«æ—¥æœ¬èªå‡¦ç†èƒ½åŠ›ãŒå‘ä¸Šã—ã€ã‚ˆã‚Šè‡ªç„¶ãªå¯¾è©±ãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚',
                'keywords': ['Google', 'Gemini', 'AIæ€§èƒ½']
            },
            {
                'title': 'Claude 3ã®ä¼æ¥­å°å…¥äº‹ä¾‹',
                'content': 'Anthropicã®Claude 3ãŒå¤šãã®ä¼æ¥­ã§å°å…¥ã•ã‚Œã¦ãŠã‚Šã€ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã‚„æ–‡æ›¸ä½œæˆã®è‡ªå‹•åŒ–ã«æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚',
                'keywords': ['Claude', 'Anthropic', 'ä¼æ¥­å°å…¥']
            },
            {
                'title': 'AIè¦åˆ¶ã«é–¢ã™ã‚‹æœ€æ–°å‹•å‘',
                'content': 'å„å›½ã§AIè¦åˆ¶ã®è­°è«–ãŒæ´»ç™ºåŒ–ã—ã¦ãŠã‚Šã€ç‰¹ã«EUã®AIæ³•æ¡ˆãŒæ³¨ç›®ã‚’é›†ã‚ã¦ã„ã¾ã™ã€‚ä¼æ¥­ã®AIæ´»ç”¨ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚',
                'keywords': ['AIè¦åˆ¶', 'EU', 'AIæ³•æ¡ˆ']
            },
            {
                'title': 'AIæŠ•è³‡å¸‚å ´ã®å‹•å‘',
                'content': 'AIã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã¸ã®æŠ•è³‡ãŒæ´»ç™ºåŒ–ã—ã¦ãŠã‚Šã€ç‰¹ã«ç”ŸæˆAIåˆ†é‡ã§ã®è³‡é‡‘èª¿é”ãŒå¢—åŠ ã—ã¦ã„ã¾ã™ã€‚',
                'keywords': ['AIæŠ•è³‡', 'ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—', 'ç”ŸæˆAI']
            }
        ]
        
        posts = []
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«2-3å€‹ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’é¸æŠ
        selected_topics = random.sample(trend_topics, min(3, len(trend_topics)))
        
        for topic in selected_topics:
            post = XPost(
                title=f"ã€Xè©±é¡Œã€‘{topic['title']}",
                url="https://x.com/search?q=" + "+".join(topic['keywords']),
                content=topic['content'],
                source="X-trend",
                published_date=datetime.now().isoformat(),
                author="AI News Collector",
                likes=random.randint(50, 500),
                retweets=random.randint(10, 100)
            )
            posts.append(post)
        
        print(f"ğŸ“Š AIé–¢é€£ãƒˆãƒ¬ãƒ³ãƒ‰: {len(posts)} ä»¶ç”Ÿæˆ")
        return posts
    
    def collect_ai_hashtag_trends(self) -> List[XPost]:
        """AIé–¢é€£ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã®ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±ã‚’åé›†"""
        hashtag_trends = [
            {
                'hashtag': '#ChatGPT',
                'description': 'ChatGPTã«é–¢ã™ã‚‹æœ€æ–°æƒ…å ±ã‚„æ´»ç”¨äº‹ä¾‹ãŒå¤šæ•°æŠ•ç¨¿ã•ã‚Œã¦ã„ã¾ã™ã€‚'
            },
            {
                'hashtag': '#AI',
                'description': 'AIå…¨èˆ¬ã«é–¢ã™ã‚‹å¹…åºƒã„è©±é¡ŒãŒè­°è«–ã•ã‚Œã¦ã„ã¾ã™ã€‚'
            },
            {
                'hashtag': '#æ©Ÿæ¢°å­¦ç¿’',
                'description': 'æ©Ÿæ¢°å­¦ç¿’ã®æŠ€è¡“ã‚„ç ”ç©¶ã«é–¢ã™ã‚‹æŠ•ç¨¿ãŒå¢—åŠ ã—ã¦ã„ã¾ã™ã€‚'
            },
            {
                'hashtag': '#ç”ŸæˆAI',
                'description': 'ç”ŸæˆAIãƒ„ãƒ¼ãƒ«ã®æ´»ç”¨æ–¹æ³•ã‚„æ–°ã‚µãƒ¼ãƒ“ã‚¹ã«ã¤ã„ã¦è©±é¡Œã¨ãªã£ã¦ã„ã¾ã™ã€‚'
            },
            {
                'hashtag': '#AIãƒ„ãƒ¼ãƒ«',
                'description': 'æ§˜ã€…ãªAIãƒ„ãƒ¼ãƒ«ã®ç´¹ä»‹ã‚„æ¯”è¼ƒæƒ…å ±ãŒå…±æœ‰ã•ã‚Œã¦ã„ã¾ã™ã€‚'
            }
        ]
        
        posts = []
        
        for trend in random.sample(hashtag_trends, 2):  # 2ã¤é¸æŠ
            post = XPost(
                title=f"ã€Xæ³¨ç›®ã€‘{trend['hashtag']} ãƒˆãƒ¬ãƒ³ãƒ‰",
                url=f"https://x.com/hashtag/{trend['hashtag'].replace('#', '')}",
                content=f"{trend['hashtag']}ãŒæ³¨ç›®ã‚’é›†ã‚ã¦ã„ã¾ã™ã€‚{trend['description']}å¤šãã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé–¢é€£ã™ã‚‹æŠ•ç¨¿ã‚’è¡Œã£ã¦ãŠã‚Šã€æ´»ç™ºãªè­°è«–ãŒå±•é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚",
                source="X-hashtag",
                published_date=datetime.now().isoformat(),
                author="Trend Monitor",
                likes=random.randint(100, 800),
                retweets=random.randint(20, 200)
            )
            posts.append(post)
        
        print(f"ğŸ·ï¸  ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ãƒˆãƒ¬ãƒ³ãƒ‰: {len(posts)} ä»¶ç”Ÿæˆ")
        return posts

async def collect_simple_x_posts(max_posts: int = 5) -> List[XPost]:
    """Xé–¢é€£æƒ…å ±ã‚’åé›†ï¼ˆãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼‰"""
    collector = SimpleXCollector()
    all_posts = []
    
    print("ğŸ“± Xé–¢é€£æƒ…å ±åé›†é–‹å§‹")
    
    try:
        # 1. ä»£æ›¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†
        alt_posts = collector.collect_from_alternative_sources()
        all_posts.extend(alt_posts)
        
        # 2. AIé–¢é€£ãƒˆãƒ¬ãƒ³ãƒ‰ç”Ÿæˆ
        trend_posts = collector.generate_ai_trend_summary()
        all_posts.extend(trend_posts)
        
        # 3. ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ãƒˆãƒ¬ãƒ³ãƒ‰åé›†
        hashtag_posts = collector.collect_ai_hashtag_trends()
        all_posts.extend(hashtag_posts)
        
        # æœ€å¤§ä»¶æ•°ã¾ã§åˆ¶é™
        final_posts = all_posts[:max_posts]
        
        print(f"âœ… Xé–¢é€£æƒ…å ±åé›†å®Œäº†: {len(final_posts)} ä»¶")
        return final_posts
        
    except Exception as e:
        print(f"âŒ Xé–¢é€£æƒ…å ±åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    import asyncio
    
    async def test():
        posts = await collect_simple_x_posts(max_posts=5)
        
        print(f"\nåé›†çµæœ: {len(posts)} ä»¶")
        for i, post in enumerate(posts, 1):
            print(f"{i}. {post.title}")
            print(f"   å†…å®¹: {post.content[:100]}...")
            print(f"   ã‚½ãƒ¼ã‚¹: {post.source}")
            print()
    
    asyncio.run(test()) 