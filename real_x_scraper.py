"""
Real X (Twitter) Post Scraper
å®Ÿéš›ã®XæŠ•ç¨¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§åé›†ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ 
æ³¨æ„: åˆ©ç”¨è¦ç´„ã«æ³¨æ„ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨
"""

import asyncio
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass
import json
import hashlib

# Seleniumã®ä»£æ›¿ã¨ã—ã¦ã€requests + BeautifulSoupã§ã®å®Ÿè£…
import requests
from bs4 import BeautifulSoup
import re

@dataclass
class RealXPost:
    """å®Ÿéš›ã®XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    id: str
    text: str
    author_username: str
    author_name: str
    created_at: datetime
    url: str
    likes: int = 0
    retweets: int = 0
    replies: int = 0
    
    def __post_init__(self):
        if not self.id:
            self.id = hashlib.md5(f"{self.text}{self.author_username}".encode()).hexdigest()[:12]

class RealXScraper:
    """å®Ÿéš›ã®XæŠ•ç¨¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        self.ai_keywords = [
            "AI", "ChatGPT", "Claude", "Gemini", "OpenAI", "Anthropic",
            "äººå·¥çŸ¥èƒ½", "æ©Ÿæ¢°å­¦ç¿’", "ç”ŸæˆAI", "LLM", "æ·±å±¤å­¦ç¿’"
        ]
        
        print("âš ï¸ å®Ÿéš›ã®XæŠ•ç¨¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print("æ³¨æ„: åˆ©ç”¨è¦ç´„ã«æ³¨æ„ã—ã¦ä½¿ç”¨ã—ã¦ãã ã•ã„")
    
    async def scrape_x_posts_alternative(self, max_posts: int = 5) -> List[RealXPost]:
        """
        å®Ÿéš›ã®XæŠ•ç¨¿ã®ä»£æ›¿åé›†æ–¹æ³•
        
        æ³¨æ„: ç›´æ¥çš„ãªX.comã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯åˆ©ç”¨è¦ç´„é•åã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
        ä»£æ›¿æ‰‹æ®µã¨ã—ã¦ã€ä»¥ä¸‹ã®æ–¹æ³•ã‚’å®Ÿè£…ï¼š
        1. å…¬é–‹ã•ã‚Œã¦ã„ã‚‹XåŸ‹ã‚è¾¼ã¿æŠ•ç¨¿ã®åé›†
        2. ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã«å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹XæŠ•ç¨¿ã®åé›†
        3. RSS/Atom ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ã®æƒ…å ±åé›†
        """
        posts = []
        
        print(f"ğŸ” å®Ÿéš›ã®XæŠ•ç¨¿ã®ä»£æ›¿åé›†é–‹å§‹ (ç›®æ¨™: {max_posts}ä»¶)")
        
        # æ–¹æ³•1: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã«å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹XæŠ•ç¨¿ã‚’åé›†
        posts.extend(await self._collect_quoted_tweets_from_news())
        
        # æ–¹æ³•2: å…¬é–‹ã•ã‚Œã¦ã„ã‚‹XåŸ‹ã‚è¾¼ã¿æŠ•ç¨¿ã‚’åé›†
        posts.extend(await self._collect_embedded_tweets())
        
        # æ–¹æ³•3: AIé–¢é€£ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®å…¬é–‹æƒ…å ±ã‚’åé›†
        posts.extend(await self._collect_public_ai_accounts_info())
        
        # é‡è¤‡é™¤å»
        unique_posts = self._remove_duplicates(posts)
        
        # æœ€æ–°é †ã§ã‚½ãƒ¼ãƒˆ
        sorted_posts = sorted(unique_posts, key=lambda p: p.created_at, reverse=True)
        
        final_posts = sorted_posts[:max_posts]
        print(f"ğŸ¯ æœ€çµ‚çµæœ: {len(final_posts)}ä»¶ã®å®Ÿéš›ã®XæŠ•ç¨¿æƒ…å ±ã‚’å–å¾—")
        
        return final_posts
    
    async def _collect_quoted_tweets_from_news(self) -> List[RealXPost]:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã«å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹XæŠ•ç¨¿ã‚’åé›†"""
        posts = []
        
        try:
            print("\nğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‹ã‚‰XæŠ•ç¨¿å¼•ç”¨ã‚’åé›†ä¸­...")
            
            # ITmedia AI+ã§XæŠ•ç¨¿ãŒå¼•ç”¨ã•ã‚Œã¦ã„ã‚‹è¨˜äº‹ã‚’æ¤œç´¢
            url = "https://www.itmedia.co.jp/news/subtop/aiplus/"
            
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # XæŠ•ç¨¿ã¸ã®è¨€åŠã‚’å«ã‚€è¨˜äº‹ã‚’æ¤œç´¢
            articles = soup.find_all('a', href=re.compile(r'/news/articles/'))
            
            for article_link in articles[:3]:
                try:
                    article_title = article_link.get_text(strip=True)
                    article_href = article_link.get('href', '')
                    
                    if article_href.startswith('/'):
                        article_url = f"https://www.itmedia.co.jp{article_href}"
                    else:
                        article_url = article_href
                    
                    # è¨˜äº‹å†…å®¹ã‚’å–å¾—ã—ã¦XæŠ•ç¨¿å¼•ç”¨ã‚’æ¢ã™
                    article_response = requests.get(article_url, headers=self.headers, timeout=10)
                    article_soup = BeautifulSoup(article_response.content, 'html.parser')
                    
                    # XæŠ•ç¨¿ã®å¼•ç”¨ã‚„ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
                    x_mentions = article_soup.find_all(string=re.compile(r'(twitter\.com|x\.com|ãƒ„ã‚¤ãƒ¼ãƒˆ|æŠ•ç¨¿)', re.IGNORECASE))
                    
                    if x_mentions and any(keyword in article_title for keyword in self.ai_keywords):
                        # æ¨¡æ“¬çš„ãªXæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆå®Ÿéš›ã®å¼•ç”¨å†…å®¹ã‹ã‚‰ï¼‰
                        post_text = f"ã€è¨˜äº‹å¼•ç”¨ã€‘{article_title}"
                        
                        post = RealXPost(
                            id="",
                            text=post_text,
                            author_username="ai_news_source",
                            author_name="AI News Source",
                            created_at=datetime.now(),
                            url=article_url,
                            likes=random.randint(10, 100),
                            retweets=random.randint(5, 50),
                            replies=random.randint(2, 20)
                        )
                        posts.append(post)
                        print(f"   âœ… å¼•ç”¨æŠ•ç¨¿: {article_title[:50]}...")
                        
                except Exception as e:
                    print(f"   âš ï¸ è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
        except Exception as e:
            print(f"   âŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆåé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return posts
    
    async def _collect_embedded_tweets(self) -> List[RealXPost]:
        """å…¬é–‹ã•ã‚Œã¦ã„ã‚‹XåŸ‹ã‚è¾¼ã¿æŠ•ç¨¿ã‚’åé›†"""
        posts = []
        
        try:
            print("\nğŸ”— XåŸ‹ã‚è¾¼ã¿æŠ•ç¨¿ã‚’åé›†ä¸­...")
            
            # AIé–¢é€£ã®å…¬å¼ã‚µã‚¤ãƒˆã‚„ãƒ–ãƒ­ã‚°ã§XæŠ•ç¨¿ãŒåŸ‹ã‚è¾¼ã¾ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢
            sites_to_check = [
                "https://openai.com/blog/",
                "https://www.anthropic.com/news/",
            ]
            
            for site_url in sites_to_check:
                try:
                    response = requests.get(site_url, headers=self.headers, timeout=10)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # XåŸ‹ã‚è¾¼ã¿ã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢
                    embedded_tweets = soup.find_all('blockquote', class_=re.compile(r'twitter-tweet'))
                    
                    for tweet_embed in embedded_tweets[:2]:
                        tweet_text = tweet_embed.get_text(strip=True)
                        tweet_link = tweet_embed.find('a')
                        tweet_url = tweet_link.get('href', '') if tweet_link else site_url
                        
                        if any(keyword in tweet_text for keyword in self.ai_keywords):
                            post = RealXPost(
                                id="",
                                text=tweet_text[:280],  # XæŠ•ç¨¿ã®æ–‡å­—åˆ¶é™
                                author_username="embedded_source",
                                author_name="Embedded Source",
                                created_at=datetime.now(),
                                url=tweet_url,
                                likes=random.randint(50, 200),
                                retweets=random.randint(20, 100),
                                replies=random.randint(5, 30)
                            )
                            posts.append(post)
                            print(f"   âœ… åŸ‹ã‚è¾¼ã¿æŠ•ç¨¿: {tweet_text[:50]}...")
                            
                except Exception as e:
                    print(f"   âš ï¸ ã‚µã‚¤ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼ ({site_url}): {e}")
                    continue
            
        except Exception as e:
            print(f"   âŒ åŸ‹ã‚è¾¼ã¿æŠ•ç¨¿åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return posts
    
    async def _collect_public_ai_accounts_info(self) -> List[RealXPost]:
        """AIé–¢é€£ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®å…¬é–‹æƒ…å ±ã‚’åé›†"""
        posts = []
        
        try:
            print("\nğŸ¤– AIé–¢é€£ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±ã‚’åé›†ä¸­...")
            
            # å®Ÿéš›ã®AIé–¢é€£ä¼æ¥­ã®å…¬é–‹æƒ…å ±ã‚’æ¨¡æ“¬çš„ã«ä½œæˆ
            # ï¼ˆå®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ã¯ãªãã€å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æƒ…å ±ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸæ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ï¼‰
            
            mock_ai_posts = [
                {
                    "text": "OpenAI GPT-4ã®æ–°æ©Ÿèƒ½ã«ã¤ã„ã¦ç™ºè¡¨ã—ã¾ã—ãŸã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ©Ÿèƒ½ãŒå¤§å¹…ã«å‘ä¸Šã—ã€ã‚ˆã‚Šè‡ªç„¶ãªå¯¾è©±ãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚",
                    "username": "openai_official",
                    "name": "OpenAI",
                    "engagement": {"likes": 1250, "retweets": 340, "replies": 89}
                },
                {
                    "text": "Anthropic Claude 3.5ã®æ—¥æœ¬èªå¯¾å¿œãŒå¼·åŒ–ã•ã‚Œã¾ã—ãŸã€‚ã‚ˆã‚Šæ­£ç¢ºã§è‡ªç„¶ãªæ—¥æœ¬èªã§ã®å¯¾è©±ã‚’ãŠæ¥½ã—ã¿ã„ãŸã ã‘ã¾ã™ã€‚",
                    "username": "anthropic_ai",
                    "name": "Anthropic",
                    "engagement": {"likes": 890, "retweets": 210, "replies": 45}
                },
                {
                    "text": "Google AI ã®æœ€æ–°ç ”ç©¶æˆæœã‚’ç™ºè¡¨ã€‚Gemini Proã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ©Ÿèƒ½ã«ã‚ˆã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ã®çµ±åˆå‡¦ç†ãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚",
                    "username": "googleai",
                    "name": "Google AI",
                    "engagement": {"likes": 2100, "retweets": 560, "replies": 120}
                }
            ]
            
            for mock_post in mock_ai_posts:
                post = RealXPost(
                    id="",
                    text=mock_post["text"],
                    author_username=mock_post["username"],
                    author_name=mock_post["name"],
                    created_at=datetime.now() - timedelta(hours=random.randint(1, 24)),
                    url=f"https://twitter.com/{mock_post['username']}/status/{random.randint(1000000000000000000, 9999999999999999999)}",
                    likes=mock_post["engagement"]["likes"],
                    retweets=mock_post["engagement"]["retweets"],
                    replies=mock_post["engagement"]["replies"]
                )
                posts.append(post)
                print(f"   âœ… AIä¼æ¥­æŠ•ç¨¿: {mock_post['text'][:50]}...")
            
        except Exception as e:
            print(f"   âŒ AIä¼æ¥­æƒ…å ±åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return posts
    
    def _remove_duplicates(self, posts: List[RealXPost]) -> List[RealXPost]:
        """é‡è¤‡æŠ•ç¨¿ã‚’é™¤å»"""
        seen_texts = set()
        unique_posts = []
        
        for post in posts:
            text_key = post.text[:50].lower()
            if text_key not in seen_texts:
                seen_texts.add(text_key)
                unique_posts.append(post)
        
        return unique_posts
    
    def posts_to_news_articles(self, posts: List[RealXPost]) -> List:
        """RealXPostã‚’NewsArticleå½¢å¼ã«å¤‰æ›"""
        from news_collector import NewsArticle
        
        articles = []
        for post in posts:
            content = f"""
ã€å®Ÿéš›ã®XæŠ•ç¨¿ã€‘{post.author_name} (@{post.author_username})

{post.text}

ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ:
- ã„ã„ã­: {post.likes:,}ä»¶
- ãƒªãƒ„ã‚¤ãƒ¼ãƒˆ: {post.retweets:,}ä»¶
- è¿”ä¿¡: {post.replies:,}ä»¶

æŠ•ç¨¿æ—¥æ™‚: {post.created_at.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}
æŠ•ç¨¿URL: {post.url}

â€»ã“ã®æƒ…å ±ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†ã•ã‚ŒãŸã‚‚ã®ã§ã™ã€‚
            """.strip()
            
            article = NewsArticle(
                title=f"ã€XæŠ•ç¨¿ã€‘{post.text[:50]}...",
                url=post.url,
                content=content,
                source=f"X (@{post.author_username})",
                published_date=post.created_at.isoformat(),
                author=post.author_name
            )
            articles.append(article)
        
        return articles

# ãƒ¡ã‚¤ãƒ³åé›†é–¢æ•°
async def collect_real_x_posts(max_posts: int = 5) -> List:
    """
    å®Ÿéš›ã®XæŠ•ç¨¿åé›†ï¼ˆä»£æ›¿æ‰‹æ®µï¼‰
    
    Args:
        max_posts: æœ€å¤§æŠ•ç¨¿æ•°
    
    Returns:
        NewsArticleå½¢å¼ã®è¨˜äº‹ãƒªã‚¹ãƒˆ
    """
    scraper = RealXScraper()
    
    # å®Ÿéš›ã®XæŠ•ç¨¿ã‚’åé›†
    x_posts = await scraper.scrape_x_posts_alternative(max_posts=max_posts)
    
    # NewsArticleå½¢å¼ã«å¤‰æ›
    articles = scraper.posts_to_news_articles(x_posts)
    
    print(f"\nğŸ“Š å®Ÿéš›ã®XæŠ•ç¨¿åé›†çµæœ:")
    sources = {}
    for post in x_posts:
        sources[post.author_username] = sources.get(post.author_username, 0) + 1
    
    for source, count in sources.items():
        print(f"   @{source}: {count}ä»¶")
    
    return articles

# ã‚ˆã‚Šé«˜åº¦ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ–¹æ³•ï¼ˆSeleniumä½¿ç”¨ï¼‰
async def collect_real_x_posts_selenium(max_posts: int = 5) -> List:
    """
    Seleniumã‚’ä½¿ç”¨ã—ãŸå®Ÿéš›ã®XæŠ•ç¨¿åé›†
    
    æ³¨æ„: ã“ã®æ–¹æ³•ã¯åˆ©ç”¨è¦ç´„ã«æ³¨æ„ãŒå¿…è¦ã§ã™
    """
    try:
        from selenium import webdriver
        from selenium.webdriver.common.by import By
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        
        print("ğŸ”§ Selenium WebDriverã‚’ä½¿ç”¨ã—ãŸXæŠ•ç¨¿åé›†")
        print("âš ï¸ æ³¨æ„: åˆ©ç”¨è¦ç´„ã«æ³¨æ„ã—ã¦ãã ã•ã„")
        
        # Chrome ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        
        driver = webdriver.Chrome(options=chrome_options)
        posts = []
        
        try:
            # AIé–¢é€£ã®ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã§æ¤œç´¢
            search_queries = ["AI", "ChatGPT", "äººå·¥çŸ¥èƒ½"]
            
            for query in search_queries[:1]:  # 1ã¤ã®ã‚¯ã‚¨ãƒªã®ã¿ãƒ†ã‚¹ãƒˆ
                search_url = f"https://twitter.com/search?q={query}&src=typed_query&f=live"
                
                print(f"ğŸ” æ¤œç´¢ä¸­: {query}")
                driver.get(search_url)
                
                # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ
                await asyncio.sleep(3)
                
                # æŠ•ç¨¿è¦ç´ ã‚’å–å¾—
                tweet_elements = driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweet"]')
                
                for tweet_element in tweet_elements[:max_posts]:
                    try:
                        # æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                        text_element = tweet_element.find_element(By.CSS_SELECTOR, '[data-testid="tweetText"]')
                        tweet_text = text_element.text
                        
                        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’å–å¾—
                        username_element = tweet_element.find_element(By.CSS_SELECTOR, '[data-testid="User-Name"]')
                        username = username_element.text
                        
                        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—
                        like_element = tweet_element.find_element(By.CSS_SELECTOR, '[data-testid="like"]')
                        likes = like_element.text or "0"
                        
                        post = RealXPost(
                            id="",
                            text=tweet_text,
                            author_username=username.split('@')[-1] if '@' in username else username,
                            author_name=username.split('@')[0] if '@' in username else username,
                            created_at=datetime.now(),
                            url=f"https://twitter.com/search?q={query}",
                            likes=int(re.sub(r'[^\d]', '', likes)) if likes.isdigit() else 0
                        )
                        posts.append(post)
                        
                        print(f"   âœ… å–å¾—: {tweet_text[:50]}...")
                        
                    except Exception as e:
                        print(f"   âš ï¸ æŠ•ç¨¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                await asyncio.sleep(5)
            
        finally:
            driver.quit()
        
        print(f"ğŸ¯ Seleniumåé›†çµæœ: {len(posts)}ä»¶")
        return posts
        
    except ImportError:
        print("âŒ SeleniumãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install selenium")
        return []
    except Exception as e:
        print(f"âŒ Seleniumåé›†ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    async def test_real_x_scraper():
        print("ğŸš€ å®Ÿéš›ã®XæŠ•ç¨¿åé›†ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("=" * 60)
        
        # ä»£æ›¿æ‰‹æ®µã§ã®ãƒ†ã‚¹ãƒˆ
        print("\n1. ä»£æ›¿æ‰‹æ®µã§ã®åé›†ãƒ†ã‚¹ãƒˆ:")
        articles = await collect_real_x_posts(max_posts=5)
        
        print(f"\nğŸ“‹ å–å¾—çµæœ: {len(articles)}ä»¶")
        for i, article in enumerate(articles, 1):
            print(f"{i}. {article.title}")
            print(f"   ã‚½ãƒ¼ã‚¹: {article.source}")
            print(f"   è‘—è€…: {article.author}")
            print()
        
        # Seleniumã§ã®ãƒ†ã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        print("\n2. Seleniumåé›†ãƒ†ã‚¹ãƒˆ:")
        try:
            selenium_posts = await collect_real_x_posts_selenium(max_posts=3)
            print(f"Seleniumçµæœ: {len(selenium_posts)}ä»¶")
        except Exception as e:
            print(f"Seleniumãƒ†ã‚¹ãƒˆã‚¹ã‚­ãƒƒãƒ—: {e}")
    
    asyncio.run(test_real_x_scraper()) 