"""
é«˜åº¦ãªWebã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
- ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½
- å¼·åŒ–ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
- User-Agentãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
"""

import asyncio
import aiohttp
import json
import os
import time
import hashlib
import random
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any
from dataclasses import dataclass, asdict
from bs4 import BeautifulSoup
import logging
from pathlib import Path

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class AdvancedArticle:
    title: str
    url: str
    content: str
    source: str
    published_date: Optional[str] = None
    scraped_at: str = None
    cache_key: str = None
    
    def __post_init__(self):
        if self.scraped_at is None:
            self.scraped_at = datetime.now().isoformat()
        if self.cache_key is None:
            self.cache_key = hashlib.md5(self.url.encode()).hexdigest()

class AdvancedScraper:
    def __init__(self, cache_dir: str = "cache", max_concurrent: int = 10):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.max_concurrent = max_concurrent
        self.session = None
        
        # User-Agentãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (X11; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.59',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15'
        ]
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¨­å®šï¼ˆã‚µã‚¤ãƒˆã”ã¨ï¼‰
        self.rate_limits = {
            'default': 1.0,  # 1ç§’é–“éš”
            'aggressive': 0.5,  # 0.5ç§’é–“éš”
            'conservative': 2.0,  # 2ç§’é–“éš”
            'premium': 0.1  # 0.1ç§’é–“éš”ï¼ˆãƒ—ãƒ¬ãƒŸã‚¢ãƒ ã‚µã‚¤ãƒˆç”¨ï¼‰
        }
        
        # ã‚µã‚¤ãƒˆåˆ¥è¨­å®š
        self.site_configs = self._load_site_configs()
        
        # æœ€å¾Œã®ã‚¢ã‚¯ã‚»ã‚¹æ™‚é–“è¨˜éŒ²
        self.last_access = {}
    
    def _load_site_configs(self) -> Dict:
        """ã‚µã‚¤ãƒˆåˆ¥è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        from simple_scraper import SimpleScraper
        simple_scraper = SimpleScraper()
        return simple_scraper.sources
    
    def _get_random_user_agent(self) -> str:
        """ãƒ©ãƒ³ãƒ€ãƒ ãªUser-Agentã‚’å–å¾—"""
        return random.choice(self.user_agents)
    
    def _get_cache_path(self, cache_key: str) -> Path:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—"""
        return self.cache_dir / f"{cache_key}.json"
    
    def _is_cache_valid(self, cache_path: Path, max_age_hours: int = 6) -> bool:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        if not cache_path.exists():
            return False
        
        file_time = datetime.fromtimestamp(cache_path.stat().st_mtime)
        return datetime.now() - file_time < timedelta(hours=max_age_hours)
    
    def _load_from_cache(self, cache_key: str) -> Optional[AdvancedArticle]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰è¨˜äº‹ã‚’èª­ã¿è¾¼ã¿"""
        cache_path = self._get_cache_path(cache_key)
        
        if self._is_cache_valid(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return AdvancedArticle(**data)
            except Exception as e:
                logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {cache_key}: {e}")
        
        return None
    
    def _save_to_cache(self, article: AdvancedArticle):
        """è¨˜äº‹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            cache_path = self._get_cache_path(article.cache_key)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(asdict(article), f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼ {article.cache_key}: {e}")
    
    async def _wait_for_rate_limit(self, site_key: str):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é©ç”¨"""
        rate_limit = self.rate_limits.get('default', 1.0)
        
        # ã‚µã‚¤ãƒˆå›ºæœ‰ã®è¨­å®šãŒã‚ã‚Œã°ä½¿ç”¨
        if site_key in self.site_configs:
            rate_type = self.site_configs[site_key].get('rate_limit', 'default')
            rate_limit = self.rate_limits.get(rate_type, rate_limit)
        
        last_time = self.last_access.get(site_key, 0)
        elapsed = time.time() - last_time
        
        if elapsed < rate_limit:
            wait_time = rate_limit - elapsed
            await asyncio.sleep(wait_time)
        
        self.last_access[site_key] = time.time()
    
    async def _get_page_content(self, url: str, timeout: int = 15) -> Optional[BeautifulSoup]:
        """ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’éåŒæœŸã§å–å¾—"""
        try:
            headers = {
                'User-Agent': self._get_random_user_agent(),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            timeout_config = aiohttp.ClientTimeout(total=timeout)
            
            async with self.session.get(url, headers=headers, timeout=timeout_config) as response:
                if response.status == 200:
                    content = await response.text()
                    return BeautifulSoup(content, 'html.parser')
                else:
                    logger.warning(f"HTTP {response.status}: {url}")
                    return None
                    
        except asyncio.TimeoutError:
            logger.warning(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}")
        except Exception as e:
            logger.warning(f"ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
        
        return None
    
    async def _extract_article_content(self, url: str, content_selector: str) -> str:
        """è¨˜äº‹ã®æœ¬æ–‡ã‚’éåŒæœŸã§æŠ½å‡º"""
        soup = await self._get_page_content(url)
        if not soup:
            return ""
        
        try:
            # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦è¡Œ
            selectors = content_selector.split(', ')
            content_parts = []
            
            for selector in selectors:
                elements = soup.select(selector.strip())
                for element in elements[:15]:  # æœ€åˆã®15å€‹ã¾ã§
                    text = element.get_text(strip=True)
                    if text and len(text) > 30:  # æ„å‘³ã®ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ã¿
                        content_parts.append(text)
            
            # ä¸€èˆ¬çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚‚è©¦è¡Œ
            if not content_parts:
                generic_selectors = [
                    'article p', '.content p', '.post p', '.entry-content p',
                    'main p', '.article-body p', '.post-content p', 'p'
                ]
                
                for selector in generic_selectors:
                    elements = soup.select(selector)
                    for element in elements[:20]:
                        text = element.get_text(strip=True)
                        if text and len(text) > 40:
                            content_parts.append(text)
                    if content_parts:
                        break
            
            return ' '.join(content_parts[:10])  # æœ€åˆã®10æ®µè½ã¾ã§
            
        except Exception as e:
            logger.warning(f"æœ¬æ–‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return ""
    
    async def _collect_from_source(self, source_name: str, source_config: dict) -> List[AdvancedArticle]:
        """ç‰¹å®šã®æƒ…å ±æºã‹ã‚‰è¨˜äº‹ã‚’éåŒæœŸã§åé›†"""
        articles = []
        
        try:
            logger.info(f"ğŸ” {source_name}ã‹ã‚‰è¨˜äº‹ã‚’åé›†ä¸­...")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™é©ç”¨
            await self._wait_for_rate_limit(source_name)
            
            # æ¤œç´¢ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            soup = await self._get_page_content(source_config['search_url'])
            if not soup:
                logger.warning(f"âŒ {source_name}: æ¤œç´¢ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—")
                return articles
            
            # è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’å–å¾—
            article_links = soup.select(source_config['article_selector'])
            
            # ä¸¦åˆ—å‡¦ç†ç”¨ã®ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
            tasks = []
            max_articles = source_config.get('max_articles', 3)
            
            for i, link in enumerate(article_links[:max_articles * 2]):  # ä½™åˆ†ã«å–å¾—
                if len(tasks) >= max_articles:
                    break
                
                try:
                    href = link.get('href')
                    if not href or not isinstance(href, str):
                        continue
                    
                    # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                    if href.startswith('/'):
                        article_url = source_config['base_url'] + href
                    elif href.startswith('http'):
                        article_url = href
                    else:
                        continue
                    
                    title = link.get_text(strip=True)
                    if not title or len(title) < 10:
                        continue
                    
                    # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
                    ai_keywords = [
                        'AI', 'ai', 'Ai', 'äººå·¥çŸ¥èƒ½', 'ChatGPT', 'GPT', 'LLM', 
                        'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'OpenAI', 'Google', 'Meta',
                        'Claude', 'Gemini', 'Anthropic', 'ç”ŸæˆAI', 'Copilot',
                        'Microsoft', 'Amazon', 'Tesla', 'NVIDIA', 'robot', 'Robot',
                        'automation', 'neural', 'algorithm', 'data science'
                    ]
                    
                    if not any(keyword in title for keyword in ai_keywords):
                        continue
                    
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
                    cache_key = hashlib.md5(article_url.encode()).hexdigest()
                    cached_article = self._load_from_cache(cache_key)
                    
                    if cached_article:
                        logger.info(f"ğŸ“‹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—: {title[:50]}...")
                        articles.append(cached_article)
                        continue
                    
                    # éåŒæœŸã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
                    task = self._process_article(article_url, title, source_name, source_config)
                    tasks.append(task)
                    
                except Exception as e:
                    logger.warning(f"è¨˜äº‹ãƒªãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            # ä¸¦åˆ—å®Ÿè¡Œ
            if tasks:
                logger.info(f"âš¡ {len(tasks)}ä»¶ã®è¨˜äº‹ã‚’ä¸¦åˆ—å‡¦ç†ä¸­...")
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in results:
                    if isinstance(result, AdvancedArticle):
                        articles.append(result)
                    elif isinstance(result, Exception):
                        logger.warning(f"è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {result}")
            
            logger.info(f"âœ… {source_name}: {len(articles)} ä»¶ã®è¨˜äº‹ã‚’åé›†")
            return articles
            
        except Exception as e:
            logger.error(f"âŒ {source_name}åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return articles
    
    async def _process_article(self, url: str, title: str, source: str, source_config: dict) -> Optional[AdvancedArticle]:
        """å€‹åˆ¥è¨˜äº‹ã‚’å‡¦ç†"""
        try:
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™é©ç”¨
            await self._wait_for_rate_limit(source)
            
            logger.info(f"ğŸ“„ è¨˜äº‹å‡¦ç†ä¸­: {title[:50]}...")
            
            # æœ¬æ–‡ã‚’å–å¾—
            content = await self._extract_article_content(url, source_config['content_selector'])
            
            if not content or len(content) < 100:
                logger.warning(f"âš ï¸  æœ¬æ–‡ãŒçŸ­ã™ãã¾ã™: {title[:30]}...")
                return None
            
            # è¨˜äº‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            article = AdvancedArticle(
                title=title,
                url=url,
                content=content,
                source=source,
                published_date=datetime.now().isoformat()
            )
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self._save_to_cache(article)
            
            logger.info(f"âœ… è¨˜äº‹å‡¦ç†å®Œäº†: {title[:50]}...")
            return article
            
        except Exception as e:
            logger.warning(f"è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return None
    
    async def collect_all_articles(self, max_total_articles: int = 20) -> List[AdvancedArticle]:
        """å…¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰è¨˜äº‹ã‚’ä¸¦åˆ—åé›†"""
        all_articles = []
        
        logger.info("ğŸš€ é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
        connector = aiohttp.TCPConnector(limit=self.max_concurrent)
        timeout = aiohttp.ClientTimeout(total=30)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            self.session = session
            
            # ä¸¦åˆ—å‡¦ç†ç”¨ã®ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
            tasks = []
            
            for source_name, source_config in self.site_configs.items():
                task = self._collect_from_source(source_name, source_config)
                tasks.append(task)
            
            # ã‚»ãƒãƒ•ã‚©ã§åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™
            semaphore = asyncio.Semaphore(self.max_concurrent)
            
            async def limited_task(task):
                async with semaphore:
                    return await task
            
            # ä¸¦åˆ—å®Ÿè¡Œ
            logger.info(f"âš¡ {len(tasks)}ã‚µã‚¤ãƒˆã‚’ä¸¦åˆ—å‡¦ç†ä¸­...")
            results = await asyncio.gather(*[limited_task(task) for task in tasks], return_exceptions=True)
            
            # çµæœã‚’ã¾ã¨ã‚ã‚‹
            for result in results:
                if isinstance(result, list):
                    all_articles.extend(result)
                elif isinstance(result, Exception):
                    logger.warning(f"ã‚µã‚¤ãƒˆåé›†ã‚¨ãƒ©ãƒ¼: {result}")
        
        # é‡è¤‡é™¤å»
        unique_articles = self._remove_duplicates(all_articles)
        
        # æœ€å¤§ä»¶æ•°ã¾ã§åˆ¶é™
        final_articles = unique_articles[:max_total_articles]
        
        logger.info(f"ğŸ‰ é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(final_articles)}ä»¶ã®è¨˜äº‹ã‚’åé›†")
        return final_articles
    
    def _remove_duplicates(self, articles: List[AdvancedArticle]) -> List[AdvancedArticle]:
        """é‡è¤‡è¨˜äº‹ã‚’é™¤å»"""
        seen_urls = set()
        seen_titles = set()
        unique_articles = []
        
        for article in articles:
            # URLã¨ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸¡æ–¹ã§ãƒã‚§ãƒƒã‚¯
            title_key = article.title.lower().strip()
            
            if article.url not in seen_urls and title_key not in seen_titles:
                seen_urls.add(article.url)
                seen_titles.add(title_key)
                unique_articles.append(article)
        
        logger.info(f"ğŸ“Š é‡è¤‡é™¤å»: {len(articles)} â†’ {len(unique_articles)} ä»¶")
        return unique_articles
    
    def clear_cache(self, max_age_days: int = 7):
        """å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤"""
        try:
            cutoff_time = datetime.now() - timedelta(days=max_age_days)
            deleted_count = 0
            
            for cache_file in self.cache_dir.glob("*.json"):
                file_time = datetime.fromtimestamp(cache_file.stat().st_mtime)
                if file_time < cutoff_time:
                    cache_file.unlink()
                    deleted_count += 1
            
            logger.info(f"ğŸ—‘ï¸  å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤: {deleted_count} ä»¶")
            
        except Exception as e:
            logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

# ä½¿ç”¨ä¾‹
async def main():
    scraper = AdvancedScraper(max_concurrent=15)
    
    # å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤
    scraper.clear_cache(max_age_days=1)
    
    # è¨˜äº‹ã‚’åé›†
    articles = await scraper.collect_all_articles(max_total_articles=25)
    
    print(f"åé›†å®Œäº†: {len(articles)} ä»¶")
    for article in articles[:5]:  # æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º
        print(f"- {article.title} ({article.source})")

if __name__ == "__main__":
    asyncio.run(main()) 