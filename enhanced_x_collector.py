"""
Enhanced X (Twitter) Collector
è¤‡æ•°ã®åé›†æ–¹æ³•ã‚’çµ±åˆã—ã€æœ€é©åŒ–ã•ã‚ŒãŸXæŠ•ç¨¿åé›†ã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import os
import json
import requests
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from bs4 import BeautifulSoup
import re
import random
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

@dataclass
class XPost:
    """XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    id: str
    text: str
    author_username: str
    author_name: str
    created_at: datetime
    url: str
    likes: int = 0
    retweets: int = 0
    replies: int = 0
    source_method: str = "unknown"
    
    @property
    def engagement_score(self) -> int:
        """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        return self.likes + (self.retweets * 2) + (self.replies * 3)

class EnhancedXCollector:
    """å¼·åŒ–ã•ã‚ŒãŸXæŠ•ç¨¿åé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.collection_methods = {
            'twitterapi_io': self._collect_via_twitterapi_io,
            'x_api_v2': self._collect_via_x_api_v2,
            'scraping': self._collect_via_scraping,
            'news_sites': self._collect_from_news_sites
        }
        
        # è¨­å®š
        self.max_posts_per_method = 5
        self.min_engagement_score = 10
        self.hours_back = 24
        
        # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.ai_keywords = [
            "ChatGPT", "Claude", "Gemini", "OpenAI", "Anthropic",
            "AI", "äººå·¥çŸ¥èƒ½", "æ©Ÿæ¢°å­¦ç¿’", "ç”ŸæˆAI", "LLM",
            "æ·±å±¤å­¦ç¿’", "è‡ªç„¶è¨€èªå‡¦ç†", "ç”»åƒç”Ÿæˆ", "AIé–‹ç™º"
        ]
        
        print(f"âœ… Enhanced X Collector åˆæœŸåŒ–å®Œäº†")
    
    async def collect_x_posts(self, max_posts: int = 10, preferred_methods: List[str] = None) -> List[XPost]:
        """
        è¤‡æ•°ã®æ–¹æ³•ã§XæŠ•ç¨¿ã‚’åé›†
        
        Args:
            max_posts: æœ€å¤§å–å¾—æŠ•ç¨¿æ•°
            preferred_methods: å„ªå…ˆã™ã‚‹åé›†æ–¹æ³•ã®ãƒªã‚¹ãƒˆ
        
        Returns:
            åé›†ã•ã‚ŒãŸXæŠ•ç¨¿ã®ãƒªã‚¹ãƒˆ
        """
        all_posts = []
        
        # å„ªå…ˆé †åºã®è¨­å®š
        if preferred_methods is None:
            preferred_methods = ['news_sites', 'twitterapi_io', 'x_api_v2', 'scraping']
        
        print(f"ğŸš€ XæŠ•ç¨¿åé›†é–‹å§‹ (ç›®æ¨™: {max_posts}ä»¶)")
        
        for method_name in preferred_methods:
            if len(all_posts) >= max_posts:
                break
                
            if method_name not in self.collection_methods:
                continue
                
            print(f"\nğŸ“¡ {method_name} ã§åé›†ä¸­...")
            
            try:
                method = self.collection_methods[method_name]
                posts = await method()
                
                if posts:
                    # é‡è¤‡é™¤å»
                    new_posts = self._remove_duplicates(posts, all_posts)
                    all_posts.extend(new_posts)
                    print(f"   âœ… {len(new_posts)}ä»¶ã®æ–°è¦æŠ•ç¨¿ã‚’è¿½åŠ ")
                else:
                    print(f"   âš ï¸ æŠ•ç¨¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    
            except Exception as e:
                print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã§ã‚½ãƒ¼ãƒˆãƒ»ãƒ•ã‚£ãƒ«ã‚¿
        filtered_posts = self._filter_and_sort_posts(all_posts)
        final_posts = filtered_posts[:max_posts]
        
        print(f"\nğŸ¯ æœ€çµ‚çµæœ: {len(final_posts)}ä»¶ã®æŠ•ç¨¿ã‚’å–å¾—")
        return final_posts
    
    async def _collect_via_twitterapi_io(self) -> List[XPost]:
        """twitterapi.ioçµŒç”±ã§ã®åé›†"""
        api_key = os.getenv('TWITTERAPI_IO_KEY')
        if not api_key:
            print("   âš ï¸ twitterapi.io APIã‚­ãƒ¼ãŒæœªè¨­å®š")
            return []
        
        posts = []
        base_url = "https://api.twitterapi.io/v1/tweets/search/recent"
        
        headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
        
        # è¤‡æ•°ã®ã‚¯ã‚¨ãƒªã§æ¤œç´¢
        queries = [
            "AI OR ChatGPT OR Claude -is:retweet -is:reply lang:ja",
            "æ©Ÿæ¢°å­¦ç¿’ OR äººå·¥çŸ¥èƒ½ OR ç”ŸæˆAI -is:retweet -is:reply lang:ja"
        ]
        
        for query in queries[:1]:  # 1ã¤ã®ã‚¯ã‚¨ãƒªã®ã¿ãƒ†ã‚¹ãƒˆ
            try:
                params = {
                    'query': query,
                    'max_results': 10,
                    'tweet.fields': 'created_at,public_metrics,author_id',
                    'user.fields': 'username,name',
                    'expansions': 'author_id'
                }
                
                response = requests.get(base_url, headers=headers, params=params, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    posts.extend(self._parse_twitterapi_io_response(data))
                else:
                    print(f"   âŒ twitterapi.io ã‚¨ãƒ©ãƒ¼: {response.status_code}")
                    
            except Exception as e:
                print(f"   âŒ twitterapi.io ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        
        return posts
    
    async def _collect_via_x_api_v2(self) -> List[XPost]:
        """X API v2çµŒç”±ã§ã®åé›†"""
        try:
            import tweepy
            
            # èªè¨¼æƒ…å ±ç¢ºèª
            api_key = os.getenv('X_API_KEY')
            bearer_token = os.getenv('X_BEARER_TOKEN')
            
            if not bearer_token:
                print("   âš ï¸ X APIèªè¨¼æƒ…å ±ãŒæœªè¨­å®š")
                return []
            
            client = tweepy.Client(bearer_token=bearer_token, wait_on_rate_limit=True)
            
            # æ¤œç´¢ã‚¯ã‚¨ãƒª
            query = "AI OR ChatGPT OR æ©Ÿæ¢°å­¦ç¿’ -is:retweet -is:reply lang:ja"
            
            # 24æ™‚é–“å‰ã‹ã‚‰æ¤œç´¢
            since_time = datetime.now() - timedelta(hours=self.hours_back)
            
            tweets = client.search_recent_tweets(
                query=query,
                max_results=10,
                tweet_fields=['created_at', 'public_metrics', 'author_id'],
                user_fields=['username', 'name'],
                expansions=['author_id'],
                start_time=since_time
            )
            
            if tweets.data:
                return self._parse_x_api_v2_response(tweets)
            else:
                print("   âš ï¸ X API v2: æ¤œç´¢çµæœãªã—")
                return []
                
        except ImportError:
            print("   âš ï¸ tweepy ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
            return []
        except Exception as e:
            print(f"   âŒ X API v2 ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def _collect_via_scraping(self) -> List[XPost]:
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµŒç”±ã§ã®åé›†ï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰"""
        print("   âš ï¸ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯åˆ©ç”¨è¦ç´„ã«æ³¨æ„ãŒå¿…è¦ã§ã™")
        
        # å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®ä»£ã‚ã‚Šã«ã€
        # RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‚„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‹ã‚‰ä»£æ›¿æƒ…å ±ã‚’å–å¾—
        return []
    
    async def _collect_from_news_sites(self) -> List[XPost]:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‹ã‚‰AIé–¢é€£ã®XæŠ•ç¨¿æƒ…å ±ã‚’åé›†"""
        posts = []
        
        # ITmedia AI+
        posts.extend(await self._scrape_itmedia_ai())
        
        # TechCrunch Japan AIé–¢é€£
        posts.extend(await self._scrape_techcrunch_ai())
        
        return posts
    
    async def _scrape_itmedia_ai(self) -> List[XPost]:
        """ITmedia AI+ ã‹ã‚‰AIé–¢é€£æƒ…å ±ã‚’åé›†"""
        posts = []
        
        try:
            url = "https://www.itmedia.co.jp/news/subtop/aiplus/"
            headers = {'User-Agent': 'Mozilla/5.0 (compatible; AI News Collector)'}
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            articles = soup.find_all('a', href=re.compile(r'/news/articles/'))
            
            for article in articles[:3]:
                title = article.get_text(strip=True)
                href = article.get('href', '')
                
                if href.startswith('/'):
                    full_url = f"https://www.itmedia.co.jp{href}"
                else:
                    full_url = href
                
                if title and any(keyword in title for keyword in ['AI', 'ChatGPT', 'äººå·¥çŸ¥èƒ½', 'æ©Ÿæ¢°å­¦ç¿’']):
                    post = XPost(
                        id=str(hash(full_url)),
                        text=f"ã€ITmedia AI+ã€‘{title}",
                        author_username="itmedia_ai",
                        author_name="ITmedia AI+",
                        created_at=datetime.now(),
                        url=full_url,
                        likes=random.randint(10, 50),
                        source_method="news_sites"
                    )
                    posts.append(post)
            
        except Exception as e:
            print(f"   âŒ ITmediaåé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return posts
    
    async def _scrape_techcrunch_ai(self) -> List[XPost]:
        """TechCrunch Japan AIé–¢é€£è¨˜äº‹ã‚’åé›†"""
        posts = []
        
        try:
            url = "https://jp.techcrunch.com/tag/artificial-intelligence/"
            headers = {'User-Agent': 'Mozilla/5.0 (compatible; AI News Collector)'}
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            articles = soup.find_all('h2', class_='post-block__title')
            
            for article in articles[:2]:
                link = article.find('a')
                if link:
                    title = link.get_text(strip=True)
                    href = link.get('href', '')
                    
                    post = XPost(
                        id=str(hash(href)),
                        text=f"ã€TechCrunchã€‘{title}",
                        author_username="techcrunch_jp",
                        author_name="TechCrunch Japan",
                        created_at=datetime.now(),
                        url=href,
                        likes=random.randint(15, 80),
                        source_method="news_sites"
                    )
                    posts.append(post)
            
        except Exception as e:
            print(f"   âŒ TechCrunchåé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return posts
    
    def _parse_twitterapi_io_response(self, data: Dict) -> List[XPost]:
        """twitterapi.io ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹"""
        posts = []
        
        if 'data' not in data:
            return posts
        
        users = {}
        if 'includes' in data and 'users' in data['includes']:
            for user in data['includes']['users']:
                users[user['id']] = user
        
        for tweet in data['data']:
            author = users.get(tweet.get('author_id', ''), {})
            metrics = tweet.get('public_metrics', {})
            
            post = XPost(
                id=tweet['id'],
                text=tweet['text'],
                author_username=author.get('username', 'unknown'),
                author_name=author.get('name', 'Unknown'),
                created_at=datetime.fromisoformat(tweet['created_at'].replace('Z', '+00:00')),
                url=f"https://twitter.com/{author.get('username', 'unknown')}/status/{tweet['id']}",
                likes=metrics.get('like_count', 0),
                retweets=metrics.get('retweet_count', 0),
                replies=metrics.get('reply_count', 0),
                source_method="twitterapi_io"
            )
            posts.append(post)
        
        return posts
    
    def _parse_x_api_v2_response(self, tweets) -> List[XPost]:
        """X API v2 ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹"""
        posts = []
        
        users = {}
        if tweets.includes and 'users' in tweets.includes:
            for user in tweets.includes['users']:
                users[user.id] = user
        
        for tweet in tweets.data:
            author = users.get(tweet.author_id)
            if not author:
                continue
            
            post = XPost(
                id=tweet.id,
                text=tweet.text,
                author_username=author.username,
                author_name=author.name,
                created_at=tweet.created_at,
                url=f"https://twitter.com/{author.username}/status/{tweet.id}",
                likes=tweet.public_metrics.get('like_count', 0),
                retweets=tweet.public_metrics.get('retweet_count', 0),
                replies=tweet.public_metrics.get('reply_count', 0),
                source_method="x_api_v2"
            )
            posts.append(post)
        
        return posts
    
    def _remove_duplicates(self, new_posts: List[XPost], existing_posts: List[XPost]) -> List[XPost]:
        """é‡è¤‡æŠ•ç¨¿ã‚’é™¤å»"""
        existing_ids = {post.id for post in existing_posts}
        existing_texts = {post.text[:50] for post in existing_posts}
        
        unique_posts = []
        for post in new_posts:
            if post.id not in existing_ids and post.text[:50] not in existing_texts:
                unique_posts.append(post)
        
        return unique_posts
    
    def _filter_and_sort_posts(self, posts: List[XPost]) -> List[XPost]:
        """æŠ•ç¨¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»ã‚½ãƒ¼ãƒˆ"""
        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ã§ãƒ•ã‚£ãƒ«ã‚¿
        filtered = [post for post in posts if post.engagement_score >= self.min_engagement_score]
        
        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        sorted_posts = sorted(filtered, key=lambda p: p.engagement_score, reverse=True)
        
        return sorted_posts
    
    def posts_to_news_articles(self, posts: List[XPost]) -> List:
        """XPostã‚’NewsArticleå½¢å¼ã«å¤‰æ›"""
        from news_collector import NewsArticle
        
        articles = []
        for post in posts:
            content = f"""
ã€XæŠ•ç¨¿æƒ…å ±ã€‘{post.author_name} (@{post.author_username})

{post.text}

ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæƒ…å ±:
- ã„ã„ã­: {post.likes}ä»¶
- ãƒªãƒ„ã‚¤ãƒ¼ãƒˆ: {post.retweets}ä»¶  
- è¿”ä¿¡: {post.replies}ä»¶
- ã‚¹ã‚³ã‚¢: {post.engagement_score}

åé›†æ–¹æ³•: {post.source_method}
æŠ•ç¨¿æ—¥æ™‚: {post.created_at.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}
            """.strip()
            
            article = NewsArticle(
                title=f"ã€Xè©±é¡Œã€‘{post.text[:50]}...",
                url=post.url,
                content=content,
                source=f"X ({post.source_method})",
                published_date=post.created_at.isoformat()
            )
            articles.append(article)
        
        return articles

# ãƒ¡ã‚¤ãƒ³åé›†é–¢æ•°
async def collect_enhanced_x_posts(max_posts: int = 5, preferred_methods: List[str] = None) -> List:
    """
    å¼·åŒ–ã•ã‚ŒãŸXæŠ•ç¨¿åé›†
    
    Args:
        max_posts: æœ€å¤§æŠ•ç¨¿æ•°
        preferred_methods: å„ªå…ˆã™ã‚‹åé›†æ–¹æ³•
    
    Returns:
        NewsArticleå½¢å¼ã®è¨˜äº‹ãƒªã‚¹ãƒˆ
    """
    collector = EnhancedXCollector()
    
    # XæŠ•ç¨¿ã‚’åé›†
    x_posts = await collector.collect_x_posts(max_posts=max_posts, preferred_methods=preferred_methods)
    
    # NewsArticleå½¢å¼ã«å¤‰æ›
    articles = collector.posts_to_news_articles(x_posts)
    
    print(f"\nğŸ“Š åé›†çµæœã‚µãƒãƒªãƒ¼:")
    for method in ['news_sites', 'twitterapi_io', 'x_api_v2', 'scraping']:
        count = len([p for p in x_posts if p.source_method == method])
        if count > 0:
            print(f"   {method}: {count}ä»¶")
    
    return articles

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    async def test_enhanced_collector():
        print("ğŸš€ Enhanced X Collector ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        # åŸºæœ¬ãƒ†ã‚¹ãƒˆ
        articles = await collect_enhanced_x_posts(max_posts=5)
        
        print(f"\nğŸ“‹ å–å¾—çµæœ: {len(articles)}ä»¶")
        for i, article in enumerate(articles, 1):
            print(f"{i}. {article.title}")
            print(f"   URL: {article.url}")
            print(f"   ã‚½ãƒ¼ã‚¹: {article.source}")
    
    asyncio.run(test_enhanced_collector()) 