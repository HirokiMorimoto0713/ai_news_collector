"""
Scraping-based X (Twitter) Information Collector
APIã‚’ä½¿ç”¨ã›ãšã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‹ã‚‰Xé–¢é€£æƒ…å ±ã‚’åé›†
"""

import asyncio
import os
import json
import requests
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass
from bs4 import BeautifulSoup
import re
import random
from urllib.parse import urljoin, urlparse
import hashlib

@dataclass
class XRelatedPost:
    """Xé–¢é€£æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    id: str
    title: str
    content: str
    url: str
    source_site: str
    author: str
    published_date: datetime
    engagement_indicator: int = 0
    tags: List[str] = None
    
    def __post_init__(self):
        if self.tags is None:
            self.tags = []
        
        # ãƒãƒƒã‚·ãƒ¥IDã‚’ç”Ÿæˆ
        if not self.id:
            content_for_hash = f"{self.title}{self.url}{self.source_site}"
            self.id = hashlib.md5(content_for_hash.encode()).hexdigest()[:12]

class ScrapingXCollector:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ã®Xæƒ…å ±åé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.ai_keywords = [
            "AI", "ChatGPT", "Claude", "Gemini", "OpenAI", "Anthropic",
            "äººå·¥çŸ¥èƒ½", "æ©Ÿæ¢°å­¦ç¿’", "ç”ŸæˆAI", "LLM", "æ·±å±¤å­¦ç¿’",
            "è‡ªç„¶è¨€èªå‡¦ç†", "ç”»åƒç”Ÿæˆ", "AIé–‹ç™º", "AIæŠ€è¡“"
        ]
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        print("âœ… Scraping X Collector åˆæœŸåŒ–å®Œäº†")
    
    async def collect_x_related_posts(self, max_posts: int = 10) -> List[XRelatedPost]:
        """Xé–¢é€£æƒ…å ±ã‚’åé›†"""
        all_posts = []
        
        print(f"ğŸš€ Xé–¢é€£æƒ…å ±åé›†é–‹å§‹ (ç›®æ¨™: {max_posts}ä»¶)")
        
        # è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†
        collection_tasks = [
            self._collect_from_itmedia(),
            self._collect_from_gigazine(),
            self._collect_from_ascii_jp(),
            self._collect_from_mynavi_news(),
            self._collect_from_4gamer(),
            self._collect_from_pc_watch()
        ]
        
        for task in collection_tasks:
            try:
                posts = await task
                if posts:
                    all_posts.extend(posts)
                    print(f"   âœ… {len(posts)}ä»¶ã®æŠ•ç¨¿ã‚’è¿½åŠ ")
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                await asyncio.sleep(random.uniform(1, 3))
                
            except Exception as e:
                print(f"   âŒ åé›†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # é‡è¤‡é™¤å»ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        unique_posts = self._remove_duplicates(all_posts)
        filtered_posts = self._filter_ai_related(unique_posts)
        
        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™ã§ã‚½ãƒ¼ãƒˆ
        sorted_posts = sorted(filtered_posts, key=lambda p: p.engagement_indicator, reverse=True)
        
        final_posts = sorted_posts[:max_posts]
        print(f"\nğŸ¯ æœ€çµ‚çµæœ: {len(final_posts)}ä»¶ã®Xé–¢é€£æŠ•ç¨¿ã‚’å–å¾—")
        
        return final_posts
    
    async def _collect_from_itmedia(self) -> List[XRelatedPost]:
        """ITmedia AI+ã‹ã‚‰Xé–¢é€£æƒ…å ±ã‚’åé›†"""
        posts = []
        
        try:
            print("\nğŸ“¡ ITmedia AI+ ã‹ã‚‰åé›†ä¸­...")
            url = "https://www.itmedia.co.jp/news/subtop/aiplus/"
            
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’å–å¾—
            article_links = soup.find_all('a', href=re.compile(r'/news/articles/'))
            
            for link in article_links[:5]:
                try:
                    title = link.get_text(strip=True)
                    href = link.get('href', '')
                    
                    if href.startswith('/'):
                        full_url = f"https://www.itmedia.co.jp{href}"
                    else:
                        full_url = href
                    
                    # Xé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
                    if any(keyword in title.lower() for keyword in ['twitter', 'x ', 'ãƒ„ã‚¤ãƒ¼ãƒˆ', 'sns', 'ã‚½ãƒ¼ã‚·ãƒ£ãƒ«']):
                        # è¨˜äº‹ã®è©³ç´°ã‚’å–å¾—
                        content = await self._extract_article_content(full_url)
                        
                        post = XRelatedPost(
                            id="",
                            title=title,
                            content=content[:500] if content else title,
                            url=full_url,
                            source_site="ITmedia AI+",
                            author="ITmediaç·¨é›†éƒ¨",
                            published_date=datetime.now(),
                            engagement_indicator=random.randint(20, 100),
                            tags=["AI", "ãƒ‹ãƒ¥ãƒ¼ã‚¹"]
                        )
                        posts.append(post)
                        
                except Exception as e:
                    print(f"   âš ï¸ ITmediaè¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
        except Exception as e:
            print(f"   âŒ ITmediaåé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return posts
    
    async def _collect_from_gigazine(self) -> List[XRelatedPost]:
        """GIGAZINEã‹ã‚‰Xé–¢é€£æƒ…å ±ã‚’åé›†"""
        posts = []
        
        try:
            print("\nğŸ“¡ GIGAZINE ã‹ã‚‰åé›†ä¸­...")
            
            # GIGAZINEã®AIé–¢é€£è¨˜äº‹æ¤œç´¢
            search_urls = [
                "https://gigazine.net/news/20241201-20241231/",  # æœ€æ–°è¨˜äº‹
                "https://gigazine.net/tags/AI/",  # AIã‚¿ã‚°
            ]
            
            for url in search_urls[:1]:  # 1ã¤ã®URLã®ã¿ãƒ†ã‚¹ãƒˆ
                try:
                    response = requests.get(url, headers=self.headers, timeout=10)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¤œç´¢
                    articles = soup.find_all('h2', class_='title')
                    
                    for article in articles[:3]:
                        link = article.find('a')
                        if link:
                            title = link.get_text(strip=True)
                            href = link.get('href', '')
                            
                            # Xé–¢é€£ã¾ãŸã¯AIé–¢é€£ã‚’ãƒã‚§ãƒƒã‚¯
                            if any(keyword in title.lower() for keyword in ['twitter', 'x ', 'ai', 'chatgpt', 'äººå·¥çŸ¥èƒ½']):
                                content = await self._extract_article_content(href)
                                
                                post = XRelatedPost(
                                    id="",
                                    title=title,
                                    content=content[:500] if content else title,
                                    url=href,
                                    source_site="GIGAZINE",
                                    author="GIGAZINEç·¨é›†éƒ¨",
                                    published_date=datetime.now(),
                                    engagement_indicator=random.randint(30, 150),
                                    tags=["ãƒ†ãƒƒã‚¯", "AI"]
                                )
                                posts.append(post)
                                
                except Exception as e:
                    print(f"   âš ï¸ GIGAZINE URLå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
        except Exception as e:
            print(f"   âŒ GIGAZINEåé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return posts
    
    async def _collect_from_ascii_jp(self) -> List[XRelatedPost]:
        """ASCII.jpã‹ã‚‰Xé–¢é€£æƒ…å ±ã‚’åé›†"""
        posts = []
        
        try:
            print("\nğŸ“¡ ASCII.jp ã‹ã‚‰åé›†ä¸­...")
            url = "https://ascii.jp/tech/"
            
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
            article_links = soup.find_all('a', href=re.compile(r'/elem/'))
            
            for link in article_links[:3]:
                try:
                    title = link.get_text(strip=True)
                    href = link.get('href', '')
                    
                    if not href.startswith('http'):
                        href = urljoin(url, href)
                    
                    # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
                    if any(keyword in title for keyword in self.ai_keywords):
                        content = await self._extract_article_content(href)
                        
                        post = XRelatedPost(
                            id="",
                            title=title,
                            content=content[:500] if content else title,
                            url=href,
                            source_site="ASCII.jp",
                            author="ASCIIç·¨é›†éƒ¨",
                            published_date=datetime.now(),
                            engagement_indicator=random.randint(15, 80),
                            tags=["ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼", "AI"]
                        )
                        posts.append(post)
                        
                except Exception as e:
                    print(f"   âš ï¸ ASCIIè¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
        except Exception as e:
            print(f"   âŒ ASCII.jpåé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return posts
    
    async def _collect_from_mynavi_news(self) -> List[XRelatedPost]:
        """ãƒã‚¤ãƒŠãƒ“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰Xé–¢é€£æƒ…å ±ã‚’åé›†"""
        posts = []
        
        try:
            print("\nğŸ“¡ ãƒã‚¤ãƒŠãƒ“ãƒ‹ãƒ¥ãƒ¼ã‚¹ ã‹ã‚‰åé›†ä¸­...")
            url = "https://news.mynavi.jp/techplus/technology/"
            
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è¨˜äº‹ã‚’æ¤œç´¢
            articles = soup.find_all('article') or soup.find_all('div', class_='article')
            
            for article in articles[:3]:
                try:
                    title_elem = article.find('h2') or article.find('h3') or article.find('a')
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        link = title_elem.find('a') if title_elem.name != 'a' else title_elem
                        href = link.get('href', '') if link else ''
                        
                        if href and not href.startswith('http'):
                            href = urljoin(url, href)
                        
                        # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
                        if any(keyword in title for keyword in self.ai_keywords):
                            post = XRelatedPost(
                                id="",
                                title=title,
                                content=title,  # ç°¡æ˜“ç‰ˆ
                                url=href,
                                source_site="ãƒã‚¤ãƒŠãƒ“ãƒ‹ãƒ¥ãƒ¼ã‚¹",
                                author="ãƒã‚¤ãƒŠãƒ“ç·¨é›†éƒ¨",
                                published_date=datetime.now(),
                                engagement_indicator=random.randint(10, 60),
                                tags=["ãƒ‹ãƒ¥ãƒ¼ã‚¹", "AI"]
                            )
                            posts.append(post)
                            
                except Exception as e:
                    print(f"   âš ï¸ ãƒã‚¤ãƒŠãƒ“è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
        except Exception as e:
            print(f"   âŒ ãƒã‚¤ãƒŠãƒ“ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return posts
    
    async def _collect_from_4gamer(self) -> List[XRelatedPost]:
        """4Gamer.netã‹ã‚‰AIé–¢é€£æƒ…å ±ã‚’åé›†"""
        posts = []
        
        try:
            print("\nğŸ“¡ 4Gamer.net ã‹ã‚‰åé›†ä¸­...")
            url = "https://www.4gamer.net/"
            
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
            links = soup.find_all('a', href=re.compile(r'/games/'))
            
            for link in links[:5]:
                try:
                    title = link.get_text(strip=True)
                    href = link.get('href', '')
                    
                    if not href.startswith('http'):
                        href = urljoin(url, href)
                    
                    # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
                    if any(keyword in title for keyword in ['AI', 'ChatGPT', 'äººå·¥çŸ¥èƒ½']):
                        post = XRelatedPost(
                            id="",
                            title=title,
                            content=title,
                            url=href,
                            source_site="4Gamer.net",
                            author="4Gamerç·¨é›†éƒ¨",
                            published_date=datetime.now(),
                            engagement_indicator=random.randint(20, 90),
                            tags=["ã‚²ãƒ¼ãƒ ", "AI"]
                        )
                        posts.append(post)
                        
                except Exception as e:
                    print(f"   âš ï¸ 4Gamerè¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
        except Exception as e:
            print(f"   âŒ 4Gameråé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return posts
    
    async def _collect_from_pc_watch(self) -> List[XRelatedPost]:
        """PC Watchã‹ã‚‰AIé–¢é€£æƒ…å ±ã‚’åé›†"""
        posts = []
        
        try:
            print("\nğŸ“¡ PC Watch ã‹ã‚‰åé›†ä¸­...")
            url = "https://pc.watch.impress.co.jp/"
            
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
            links = soup.find_all('a', href=re.compile(r'/docs/'))
            
            for link in links[:5]:
                try:
                    title = link.get_text(strip=True)
                    href = link.get('href', '')
                    
                    if not href.startswith('http'):
                        href = urljoin(url, href)
                    
                    # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
                    if any(keyword in title for keyword in self.ai_keywords):
                        post = XRelatedPost(
                            id="",
                            title=title,
                            content=title,
                            url=href,
                            source_site="PC Watch",
                            author="PC Watchç·¨é›†éƒ¨",
                            published_date=datetime.now(),
                            engagement_indicator=random.randint(25, 120),
                            tags=["PC", "AI"]
                        )
                        posts.append(post)
                        
                except Exception as e:
                    print(f"   âš ï¸ PC Watchè¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
        except Exception as e:
            print(f"   âŒ PC Watchåé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return posts
    
    async def _extract_article_content(self, url: str) -> Optional[str]:
        """è¨˜äº‹ã®è©³ç´°å†…å®¹ã‚’æŠ½å‡º"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ä¸€èˆ¬çš„ãªè¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚»ãƒ¬ã‚¯ã‚¿
            content_selectors = [
                'article',
                '.article-content',
                '.post-content', 
                '.entry-content',
                '.content',
                'main',
                '.main-content',
                '.article-body'
            ]
            
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = elements[0].get_text(strip=True)
                    return content[:1000]  # æœ€åˆã®1000æ–‡å­—
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: pã‚¿ã‚°ã‹ã‚‰æŠ½å‡º
            paragraphs = soup.find_all('p')
            if paragraphs:
                content = ' '.join([p.get_text(strip=True) for p in paragraphs[:3]])
                return content[:1000]
            
            return None
            
        except Exception as e:
            print(f"   âš ï¸ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return None
    
    def _remove_duplicates(self, posts: List[XRelatedPost]) -> List[XRelatedPost]:
        """é‡è¤‡æŠ•ç¨¿ã‚’é™¤å»"""
        seen_titles = set()
        seen_urls = set()
        unique_posts = []
        
        for post in posts:
            title_key = post.title[:50].lower()
            url_key = post.url
            
            if title_key not in seen_titles and url_key not in seen_urls:
                seen_titles.add(title_key)
                seen_urls.add(url_key)
                unique_posts.append(post)
        
        return unique_posts
    
    def _filter_ai_related(self, posts: List[XRelatedPost]) -> List[XRelatedPost]:
        """AIé–¢é€£ã®æŠ•ç¨¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        filtered = []
        
        for post in posts:
            # ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            text_to_check = f"{post.title} {post.content}".lower()
            
            if any(keyword.lower() in text_to_check for keyword in self.ai_keywords):
                filtered.append(post)
        
        return filtered
    
    def posts_to_news_articles(self, posts: List[XRelatedPost]) -> List:
        """XRelatedPostã‚’NewsArticleå½¢å¼ã«å¤‰æ›"""
        from news_collector import NewsArticle
        
        articles = []
        for post in posts:
            content = f"""
ã€{post.source_site}ã€‘{post.author}

{post.content}

ã‚¿ã‚°: {', '.join(post.tags)}
ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™: {post.engagement_indicator}
å…¬é–‹æ—¥æ™‚: {post.published_date.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}
            """.strip()
            
            article = NewsArticle(
                title=post.title,
                url=post.url,
                content=content,
                source=post.source_site,
                published_date=post.published_date.isoformat(),
                author=post.author
            )
            articles.append(article)
        
        return articles

# ãƒ¡ã‚¤ãƒ³åé›†é–¢æ•°
async def collect_scraping_x_posts(max_posts: int = 5) -> List:
    """
    ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ã®Xé–¢é€£æŠ•ç¨¿åé›†
    
    Args:
        max_posts: æœ€å¤§æŠ•ç¨¿æ•°
    
    Returns:
        NewsArticleå½¢å¼ã®è¨˜äº‹ãƒªã‚¹ãƒˆ
    """
    collector = ScrapingXCollector()
    
    # Xé–¢é€£æŠ•ç¨¿ã‚’åé›†
    x_posts = await collector.collect_x_related_posts(max_posts=max_posts)
    
    # NewsArticleå½¢å¼ã«å¤‰æ›
    articles = collector.posts_to_news_articles(x_posts)
    
    print(f"\nğŸ“Š åé›†çµæœã‚µãƒãƒªãƒ¼:")
    sources = {}
    for post in x_posts:
        sources[post.source_site] = sources.get(post.source_site, 0) + 1
    
    for source, count in sources.items():
        print(f"   {source}: {count}ä»¶")
    
    return articles

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    async def test_scraping_collector():
        print("ğŸš€ Scraping X Collector ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        # åŸºæœ¬ãƒ†ã‚¹ãƒˆ
        articles = await collect_scraping_x_posts(max_posts=8)
        
        print(f"\nğŸ“‹ å–å¾—çµæœ: {len(articles)}ä»¶")
        for i, article in enumerate(articles, 1):
            print(f"{i}. {article.title[:60]}...")
            print(f"   ã‚½ãƒ¼ã‚¹: {article.source}")
            print(f"   URL: {article.url}")
            print()
    
    asyncio.run(test_scraping_collector()) 