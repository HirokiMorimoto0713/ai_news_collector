"""
çµ±åˆAIæƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ 
å„ç¨®ã‚½ãƒ¼ã‚¹ã‹ã‚‰AIé–¢é€£æƒ…å ±ã‚’åé›†ã—ã€çµ±åˆç®¡ç†
"""

import asyncio
import json
import os
from datetime import datetime
from typing import List, Dict
from news_collector import AINewsCollector, NewsArticle
from twitter_collector import collect_twitter_articles
from simple_scraper import SimpleScraper, SimpleArticle

class IntegratedAICollector:
    """çµ±åˆAIæƒ…å ±åé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_file: str = "collector_config.json"):
        self.config_file = config_file
        self.config = self.load_config()
        self.news_collector = AINewsCollector()
        self.simple_scraper = SimpleScraper()
        self.all_articles = []
    
    def load_config(self) -> Dict:
        """è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        default_config = {
            "max_articles_per_day": 10,
            "sources": {
                "twitter": {
                    "enabled": False,
                    "use_api": False,
                    "max_articles": 0
                },
                "news_sites": {
                    "enabled": True,
                    "max_articles": 10
                },
                "tech_blogs": {
                    "enabled": True,
                    "max_articles": 5
                },
                "simple_scraper": {
                    "enabled": True,
                    "max_articles": 5
                }
            },
            "filters": {
                "min_content_length": 100,
                "exclude_keywords": ["åºƒå‘Š", "PR", "ã‚¹ãƒãƒ³ã‚µãƒ¼"],
                "required_keywords": ["AI", "äººå·¥çŸ¥èƒ½", "æ©Ÿæ¢°å­¦ç¿’", "ChatGPT", "LLM", "æŠ€è¡“"]
            }
        }
        
        if os.path.exists(self.config_file):
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã¨ãƒãƒ¼ã‚¸
                for key, value in default_config.items():
                    if key not in config:
                        config[key] = value
                return config
        else:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, ensure_ascii=False, indent=2)
            return default_config

    def simple_article_to_news_article(self, simple_article: SimpleArticle) -> NewsArticle:
        """SimpleArticleã‚’NewsArticleã«å¤‰æ›"""
        published_date = simple_article.published_date
        if published_date is None:
            published_date = datetime.now().isoformat()
        
        return NewsArticle(
            title=simple_article.title,
            url=simple_article.url,
            content=simple_article.content,
            source=simple_article.source,
            published_date=published_date
        )
    
    def filter_article(self, article: NewsArticle) -> bool:
        """è¨˜äº‹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        filters = self.config["filters"]
        
        # æœ€å°æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
        if len(article.content) < filters["min_content_length"]:
            return False
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        content_lower = article.content.lower()
        title_lower = article.title.lower()
        
        for exclude_word in filters["exclude_keywords"]:
            if exclude_word in content_lower or exclude_word in title_lower:
                return False
        
        # å¿…é ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        has_required_keyword = False
        for required_word in filters["required_keywords"]:
            if required_word.lower() in content_lower or required_word.lower() in title_lower:
                has_required_keyword = True
                break
        
        return has_required_keyword
    
    def filter_by_time(self, articles: List[NewsArticle], hours: int = 24) -> List[NewsArticle]:
        """æŒ‡å®šæ™‚é–“ä»¥å†…ã®è¨˜äº‹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        from datetime import datetime, timedelta
        
        now = datetime.now()
        time_limit = now - timedelta(hours=hours)
        filtered_articles = []
        
        for article in articles:
            # published_dateãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
            if article.published_date:
                try:
                    # ISOå½¢å¼ã®æ—¥æ™‚ã‚’ãƒ‘ãƒ¼ã‚¹
                    published_time = datetime.fromisoformat(article.published_date.replace('Z', '+00:00'))
                    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’é™¤å»ã—ã¦æ¯”è¼ƒ
                    published_time = published_time.replace(tzinfo=None)
                    
                    if published_time >= time_limit:
                        filtered_articles.append(article)
                    else:
                        print(f"å¤ã„è¨˜äº‹ã‚’ã‚¹ã‚­ãƒƒãƒ—: {article.title} ({article.published_date})")
                except Exception as e:
                    print(f"æ—¥æ™‚è§£æã‚¨ãƒ©ãƒ¼ ({article.title}): {e}")
                    # æ—¥æ™‚ãŒè§£æã§ããªã„å ´åˆã¯å«ã‚ã‚‹ï¼ˆå®‰å…¨å´ã«å€’ã™ï¼‰
                    filtered_articles.append(article)
            else:
                # å…¬é–‹æ—¥æ™‚ãŒä¸æ˜ãªå ´åˆã¯å«ã‚ã‚‹ï¼ˆå®‰å…¨å´ã«å€’ã™ï¼‰
                filtered_articles.append(article)
        
        return filtered_articles
    
    async def collect_x_related_information(self, max_posts: int = 3) -> List[NewsArticle]:
        """Xé–¢é€£æƒ…å ±ã‚’åé›†ï¼ˆå®Œå…¨ç„¡åŠ¹åŒ–æ¸ˆã¿ï¼‰"""
        print("âš ï¸ Xé–¢é€£æƒ…å ±åé›†ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ï¼‰")
        return []
        
        # try:
        #     from simple_x_collector import collect_simple_x_posts
        #     
        #     print("ğŸ“± Xé–¢é€£æƒ…å ±åé›†é–‹å§‹...")
        #     articles = await collect_simple_x_posts(max_posts=max_posts)
        #     print(f"   âœ… Xé–¢é€£æƒ…å ±: {len(articles)}ä»¶å–å¾—")
        #     return articles
        #     
        # except Exception as e:
        #     print(f"   âŒ Xé–¢é€£æƒ…å ±åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        #     return []
    
    async def collect_all_articles(self) -> List[NewsArticle]:
        """å…¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰è¨˜äº‹ã‚’åé›†"""
        all_articles = []
        
        print("AIé–¢é€£æƒ…å ±ã®åé›†ã‚’é–‹å§‹...")
        
        # 1. é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
        try:
            from advanced_scraper import AdvancedScraper
            advanced_scraper = AdvancedScraper(max_concurrent=15)
            
            # å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤
            advanced_scraper.clear_cache(max_age_days=1)
            
            # é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
            advanced_articles = await advanced_scraper.collect_all_articles(max_total_articles=20)
            
            # AdvancedArticleã‚’NewsArticleã«å¤‰æ›
            converted_articles = []
            for adv_article in advanced_articles:
                from news_collector import NewsArticle
                article = NewsArticle(
                    title=adv_article.title,
                    url=adv_article.url,
                    content=adv_article.content,
                    source=adv_article.source,
                    published_date=adv_article.published_date or datetime.now().isoformat()
                )
                converted_articles.append(article)
            
            all_articles.extend(converted_articles)
            print(f"âš¡ é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: {len(converted_articles)}ä»¶")
            
        except Exception as e:
            print(f"âŒ é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            try:
                collector = AINewsCollector()
                regular_articles = collector.collect_daily_articles()
                all_articles.extend(regular_articles)
                print(f"ğŸ“° ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åé›†: {len(regular_articles)}ä»¶")
            except Exception as e2:
                print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åé›†ã‚¨ãƒ©ãƒ¼: {e2}")
        
        # Xé–¢é€£æƒ…å ±åé›† - å®Œå…¨ç„¡åŠ¹åŒ–
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã«ã‚ˆã‚Šã€Xé¢¨ã®æƒ…å ±ã‚‚å«ã‚ã¦å…¨ã¦ç„¡åŠ¹åŒ–
        print(f"âš ï¸ Xé–¢é€£æƒ…å ±åé›†ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ï¼‰")
        
        # 24æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
        filtered_articles = self.filter_by_time(all_articles, 24)
        print(f"ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ: {len(filtered_articles)}ä»¶")
        
        print(f"ğŸ“Š ç·åˆè¨ˆ: {len(filtered_articles)}ä»¶ã®é«˜å“è³ªè¨˜äº‹ã‚’åé›†")
        return filtered_articles
    
    def remove_duplicates(self, articles: List[NewsArticle]) -> List[NewsArticle]:
        """é‡è¤‡è¨˜äº‹ã‚’é™¤å»"""
        seen_hashes = set()
        unique_articles = []
        
        for article in articles:
            if article.hash_id not in seen_hashes:
                seen_hashes.add(article.hash_id)
                unique_articles.append(article)
        
        return unique_articles
    
    def save_collected_articles(self, filename: str = None):
        """åé›†ã—ãŸè¨˜äº‹ã‚’ä¿å­˜"""
        if not filename:
            filename = f"daily_ai_news_{datetime.now().strftime('%Y%m%d')}.json"
        
        articles_data = []
        for article in self.all_articles:
            articles_data.append({
                'title': article.title,
                'url': article.url,
                'content': article.content,
                'source': article.source,
                'published_date': article.published_date,
                'author': article.author,
                'hash_id': article.hash_id
            })
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(articles_data, f, ensure_ascii=False, indent=2)
        
        print(f"è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    def get_collection_summary(self) -> Dict:
        """åé›†ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        source_counts = {}
        for article in self.all_articles:
            source = article.source
            source_counts[source] = source_counts.get(source, 0) + 1
        
        return {
            'total_articles': len(self.all_articles),
            'source_breakdown': source_counts,
            'collection_time': datetime.now().isoformat(),
            'config_used': self.config
        }

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    collector = IntegratedAICollector()
    
    print("çµ±åˆAIæƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    articles = await collector.collect_all_articles()
    
    if articles:
        collector.save_collected_articles()
        summary = collector.get_collection_summary()
        
        print("\n=== åé›†ã‚µãƒãƒªãƒ¼ ===")
        print(f"ç·è¨˜äº‹æ•°: {summary['total_articles']}")
        print("ã‚½ãƒ¼ã‚¹åˆ¥å†…è¨³:")
        for source, count in summary['source_breakdown'].items():
            print(f"  {source}: {count}ä»¶")
        
        print("\n=== åé›†è¨˜äº‹ä¸€è¦§ ===")
        for i, article in enumerate(articles, 1):
            print(f"{i}. {article.title}")
            print(f"   ã‚½ãƒ¼ã‚¹: {article.source}")
            print(f"   URL: {article.url}")
            print()
    else:
        print("è¨˜äº‹ãŒåé›†ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

async def collect_all_ai_news():
    """ã™ã¹ã¦ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
    all_articles = []
    
    print("ğŸš€ çµ±åˆAIãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹")
    print("=" * 60)
    
    # 1. é€šå¸¸ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã®ã¿
    try:
        print("\nğŸ“° é€šå¸¸ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ä¸­...")
        from news_collector import AINewsCollector
        news_articles = await collect_news()
        all_articles.extend(news_articles)
        print(f"âœ… é€šå¸¸ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(news_articles)}ä»¶")
    except Exception as e:
        print(f"âŒ é€šå¸¸ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚¨ãƒ©ãƒ¼: {e}")
    
    # Xé–¢é€£æƒ…å ±åé›†ã¯å®Œå…¨ã«ç„¡åŠ¹åŒ–
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã«ã‚ˆã‚Šã€Xé¢¨ã®æƒ…å ±ã‚‚å«ã‚ã¦å…¨ã¦ç„¡åŠ¹åŒ–
    print(f"\nâš ï¸ Xé–¢é€£æƒ…å ±åé›†ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
    print(f"   - å®Ÿéš›ã®XæŠ•ç¨¿åé›†: æŠ€è¡“çš„åˆ¶ç´„ã«ã‚ˆã‚Šä¿ç•™")
    print(f"   - Xé¢¨ã®ä»£æ›¿æƒ…å ±: ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã«ã‚ˆã‚Šç„¡åŠ¹åŒ–")
    print(f"   - åé›†å¯¾è±¡: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®ã¿")
    
    # # 2. Simple X Collectorï¼ˆå®‰å®šç‰ˆï¼‰- å®Œå…¨ç„¡åŠ¹åŒ–
    # try:
    #     print("\nğŸ¦ Xé–¢é€£æƒ…å ±åé›†ä¸­...")
    #     from simple_x_collector import collect_x_related_info
    #     x_articles = await collect_x_related_info()
    #     all_articles.extend(x_articles)
    #     print(f"âœ… Xé–¢é€£æƒ…å ±: {len(x_articles)}ä»¶")
    # except Exception as e:
    #     print(f"âŒ Xé–¢é€£æƒ…å ±åé›†ã‚¨ãƒ©ãƒ¼: {e}")
    
    # XæŠ•ç¨¿åé›†æ©Ÿèƒ½ã¯ä¸€æ—¦ä¿ç•™
    # TODO: å®Ÿéš›ã®XæŠ•ç¨¿åé›†æ©Ÿèƒ½ã®å®Ÿè£…ã‚’æ¤œè¨ä¸­
    # ç¾åœ¨ã®ä»£æ›¿æ‰‹æ®µã¯è¦æ±‚ã«åˆã‚ãªã„ãŸã‚ä¿ç•™
    
    # # 3. ç›´æ¥XæŠ•ç¨¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆå®Ÿé¨“ç‰ˆï¼‰- ä¿ç•™ä¸­
    # try:
    #     print("\nğŸ” ç›´æ¥XæŠ•ç¨¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
    #     from direct_x_scraper import collect_direct_x_posts
    #     direct_x_articles = await collect_direct_x_posts(max_posts=3)
    #     if direct_x_articles:
    #         all_articles.extend(direct_x_articles)
    #         print(f"âœ… ç›´æ¥XæŠ•ç¨¿: {len(direct_x_articles)}ä»¶")
    #     else:
    #         print("âš ï¸ ç›´æ¥XæŠ•ç¨¿: 0ä»¶ (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¸ˆã¿)")
    # except Exception as e:
    #     print(f"âŒ ç›´æ¥XæŠ•ç¨¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
    
    # # 4. Real X Scraperï¼ˆä»£æ›¿æ‰‹æ®µï¼‰- ä¿ç•™ä¸­
    # try:
    #     print("\nğŸ“± ä»£æ›¿XæŠ•ç¨¿åé›†ä¸­...")
    #     from real_x_scraper import collect_real_x_posts
    #     real_x_articles = await collect_real_x_posts(max_posts=3)
    #     all_articles.extend(real_x_articles)
    #     print(f"âœ… ä»£æ›¿XæŠ•ç¨¿: {len(real_x_articles)}ä»¶")
    # except Exception as e:
    #     print(f"âŒ ä»£æ›¿XæŠ•ç¨¿åé›†ã‚¨ãƒ©ãƒ¼: {e}")
    
    # é‡è¤‡é™¤å»
    unique_articles = []
    seen_titles = set()
    for article in all_articles:
        title_key = article.title[:50].lower()
        if title_key not in seen_titles:
            seen_titles.add(title_key)
            unique_articles.append(article)
    
    print(f"\nğŸ“Š çµ±åˆåé›†çµæœ:")
    print(f"   ç·åé›†æ•°: {len(all_articles)}ä»¶")
    print(f"   é‡è¤‡é™¤å»å¾Œ: {len(unique_articles)}ä»¶")
    print(f"   â€»åé›†å¯¾è±¡: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®ã¿")
    
    # ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ
    source_stats = {}
    for article in unique_articles:
        source = article.source
        source_stats[source] = source_stats.get(source, 0) + 1
    
    print(f"\nğŸ“ˆ ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ:")
    for source, count in source_stats.items():
        print(f"   {source}: {count}ä»¶")
    
    return unique_articles

if __name__ == "__main__":
    asyncio.run(main())

