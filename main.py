#!/usr/bin/env python3
"""
AIæƒ…å ±åé›†ãƒ»æŠ•ç¨¿ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
1æ—¥1è¨˜äº‹ã§5ã¤ã®é«˜å“è³ªãªAIè¨˜äº‹ã‚’ã¾ã¨ã‚ã¦æŠ•ç¨¿
"""

import asyncio
import json
import os
from datetime import datetime
from typing import List
from integrated_collector import IntegratedAICollector
from article_processor import ArticleProcessor, BlogPostGenerator
from wordpress_connector import WordPressConnector

class AINewsSystem:
    """AIæƒ…å ±åé›†ãƒ»æŠ•ç¨¿ã‚·ã‚¹ãƒ†ãƒ ï¼ˆ1æ—¥1è¨˜äº‹ã¾ã¨ã‚ç‰ˆï¼‰"""
    
    def __init__(self):
        self.collector = IntegratedAICollector()
        self.processor = ArticleProcessor()
        self.blog_generator = BlogPostGenerator()
        self.wp_connector = WordPressConnector()
        self.articles_per_post = 5  # 1è¨˜äº‹ã«å«ã‚ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°
        self.max_posts_per_day = 1  # 1æ—¥1è¨˜äº‹
        
    def load_today_posts_count(self) -> int:
        """ä»Šæ—¥ã®æŠ•ç¨¿æ•°ã‚’å–å¾—"""
        today = datetime.now().strftime('%Y-%m-%d')
        count_file = f"daily_posts_{today}.json"
        
        if os.path.exists(count_file):
            with open(count_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('count', 0)
        return 0
    
    def save_today_posts_count(self, count: int):
        """ä»Šæ—¥ã®æŠ•ç¨¿æ•°ã‚’ä¿å­˜"""
        today = datetime.now().strftime('%Y-%m-%d')
        count_file = f"daily_posts_{today}.json"
        
        data = {
            'date': today,
            'count': count,
            'last_updated': datetime.now().isoformat()
        }
        
        with open(count_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def select_best_articles(self, articles: List, max_count: int = 5) -> List:
        """æœ€é«˜å“è³ªã®è¨˜äº‹ã‚’é¸æŠ"""
        if len(articles) <= max_count:
            return articles
            
        # è¨˜äº‹ã®å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        scored_articles = []
        
        for article in articles:
            score = 0
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã®å“è³ªï¼ˆé•·ã•ã¨å†…å®¹ï¼‰
            title_len = len(article.title)
            if 20 <= title_len <= 80:
                score += 2
            elif title_len > 80:
                score += 1
            
            # æœ¬æ–‡ã®å“è³ªï¼ˆé•·ã•ï¼‰
            content_len = len(article.content)
            if content_len > 500:
                score += 3
            elif content_len > 300:
                score += 2
            elif content_len > 150:
                score += 1
            
            # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å­˜åœ¨
            important_keywords = [
                'ChatGPT', 'OpenAI', 'Google', 'Meta', 'Apple', 
                'Claude', 'Anthropic', 'Gemini', 'ç”ŸæˆAI', 'LLM',
                'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'äººå·¥çŸ¥èƒ½'
            ]
            
            title_content = (article.title + ' ' + article.content[:200]).lower()
            keyword_count = sum(1 for keyword in important_keywords 
                              if keyword.lower() in title_content)
            score += keyword_count
            
            # ã‚½ãƒ¼ã‚¹ã®ä¿¡é ¼æ€§
            trusted_sources = ['itmedia', 'gigazine', 'techcrunch', 'wired', 'theverge', 'arstechnica', 'venturebeat']
            if hasattr(article, 'source') and article.source.lower() in trusted_sources:
                score += 2
            
            scored_articles.append((score, article))
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’é¸æŠ
        scored_articles.sort(key=lambda x: x[0], reverse=True)
        return [article for score, article in scored_articles[:max_count]]
    
    def extract_image_from_article(self, article) -> str:
        """è¨˜äº‹ã‹ã‚‰ç”»åƒã‚’æŠ½å‡ºã€ã¾ãŸã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®ãƒ­ã‚´ã‚’è¿”ã™"""
        import re
        
        print(f"ç”»åƒæŠ½å‡ºé–‹å§‹: {article.title[:50]}...")
        
        # 1. è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ç”»åƒã‚’æ¤œç´¢
        img_patterns = [
            r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>',
            r'<img[^>]+src=([^\s>]+)[^>]*>',
            r'https://[^\s"\'<>]+\.(jpg|jpeg|png|gif|webp)',
        ]
        
        for i, pattern in enumerate(img_patterns, 1):
            matches = re.findall(pattern, article.content, re.IGNORECASE)
            if matches:
                img_url = matches[0] if isinstance(matches[0], str) else matches[0][0]
                # ç›¸å¯¾URLã®å ´åˆã¯çµ¶å¯¾URLã«å¤‰æ›
                if img_url.startswith('/'):
                    from urllib.parse import urljoin, urlparse
                    base_url = f"{urlparse(article.url).scheme}://{urlparse(article.url).netloc}"
                    img_url = urljoin(base_url, img_url)
                print(f"  è¨˜äº‹å†…ç”»åƒç™ºè¦‹ (ãƒ‘ã‚¿ãƒ¼ãƒ³{i}): {img_url}")
                return img_url
        
        print("  è¨˜äº‹å†…ã«ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        # 2. ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®ãƒ­ã‚´ã‚’ä½¿ç”¨
        source = getattr(article, 'source', '').lower()
        
        # URLã‹ã‚‰ã‚µã‚¤ãƒˆåã‚’æ¨å®šã™ã‚‹é–¢æ•°
        def get_site_from_url(url):
            if not url:
                return ''
            url = url.lower()
            
            # æ—¥æœ¬èªã‚µã‚¤ãƒˆ
            if 'itmedia' in url:
                return 'itmedia'
            elif 'gigazine' in url:
                return 'gigazine'
            elif 'pc.watch.impress' in url or 'impress' in url:
                return 'impress_watch'
            elif 'ascii.jp' in url:
                return 'ascii'
            elif 'mynavi' in url:
                return 'mynavi'
            elif 'xtech.nikkei' in url:
                return 'nikkei_xtech'
            
            # æµ·å¤–ãƒ¡ã‚¸ãƒ£ãƒ¼ã‚µã‚¤ãƒˆ
            elif 'techcrunch' in url:
                return 'techcrunch'
            elif 'venturebeat' in url:
                return 'venturebeat'
            elif 'arstechnica' in url:
                return 'arstechnica'
            elif 'theverge' in url:
                return 'theverge'
            elif 'wired' in url:
                return 'wired'
            elif 'zdnet' in url:
                return 'zdnet'
            elif 'engadget' in url:
                return 'engadget'
            
            # AIå°‚é–€ã‚µã‚¤ãƒˆ
            elif 'artificialintelligence-news' in url:
                return 'ainews'
            elif 'towardsdatascience' in url:
                return 'towards_data_science'
            elif 'machinelearningmastery' in url:
                return 'machine_learning_mastery'
            
            # ãƒ“ã‚¸ãƒã‚¹ãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆ
            elif 'reuters' in url:
                return 'reuters'
            elif 'bloomberg' in url:
                return 'bloomberg'
            elif 'cnbc' in url:
                return 'cnbc'
            elif 'cnn' in url:
                return 'cnn_business'
            
            # å­¦è¡“ãƒ»ç ”ç©¶ç³»
            elif 'technologyreview' in url:
                return 'mit_tech_review'
            elif 'nature.com' in url:
                return 'nature'
            elif 'sciencedaily' in url:
                return 'science_daily'
            
            # é–‹ç™ºè€…å‘ã‘
            elif 'dev.to' in url:
                return 'dev_to'
            elif 'news.ycombinator' in url:
                return 'hackernews'
            elif 'medium.com' in url:
                return 'medium'
            
            # ä¼æ¥­ãƒ–ãƒ­ã‚°
            elif 'openai.com' in url:
                return 'openai_blog'
            elif 'ai.googleblog' in url:
                return 'google_ai'
            elif 'blogs.microsoft' in url:
                return 'microsoft_ai'
            elif 'anthropic.com' in url:
                return 'anthropic_blog'
            
            # è¿½åŠ ã®æµ·å¤–ã‚µã‚¤ãƒˆ
            elif 'nextbigfuture' in url:
                return 'nextbigfuture'
            elif 'singularityhub' in url:
                return 'singularityhub'
            
            # ãã®ä»–
            elif 'nikkei' in url:
                return 'nikkei'
            elif 'yahoo' in url:
                return 'yahoo'
            elif 'google' in url:
                return 'google'
            elif 'bbc' in url:
                return 'bbc'
            
            return ''
        
        # sourceãŒç©ºã®å ´åˆã¯URLã‹ã‚‰æ¨å®š
        if not source:
            source = get_site_from_url(getattr(article, 'url', ''))
        
        site_logos = {
            # æ—¥æœ¬èªã‚µã‚¤ãƒˆ
            'itmedia': 'https://image.itmedia.co.jp/images/common/logo_itmedia.gif',
            'gigazine': 'https://gigazine.net/giga.png',
            'impress_watch': 'https://pc.watch.impress.co.jp/img/pcw/head/logo.png',
            'ascii': 'https://ascii.jp/img/2017/logo.svg',
            'mynavi': 'https://news.mynavi.jp/image/logo_mynavi_news.png',
            'nikkei_xtech': 'https://xtech.nikkei.com/favicon.ico',
            
            # æµ·å¤–ãƒ¡ã‚¸ãƒ£ãƒ¼ã‚µã‚¤ãƒˆ
            'techcrunch': 'https://techcrunch.com/wp-content/uploads/2015/02/cropped-cropped-favicon-gradient.png',
            'venturebeat': 'https://venturebeat.com/wp-content/themes/vb-news/img/logos/VB_Logo_Dark.svg',
            'arstechnica': 'https://cdn.arstechnica.net/wp-content/uploads/2016/10/cropped-ars-logo-512_480-32x32.png',
            'theverge': 'https://cdn.vox-cdn.com/uploads/chorus_asset/file/7395359/android-chrome-192x192.0.png',
            'wired': 'https://www.wired.com/verso/static/wired/assets/logo-wired.svg',
            'zdnet': 'https://www.zdnet.com/a/img/resize/logo-zdnet.png',
            'engadget': 'https://s.blogcdn.com/www.engadget.com/media/2013/07/favicon-192.png',
            
            # AIå°‚é–€ã‚µã‚¤ãƒˆ
            'ainews': 'https://www.artificialintelligence-news.com/favicon.ico',
            'towards_data_science': 'https://miro.medium.com/v2/resize:fill:152:152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png',
            'machine_learning_mastery': 'https://machinelearningmastery.com/favicon.ico',
            
            # ãƒ“ã‚¸ãƒã‚¹ãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆ
            'reuters': 'https://www.reuters.com/pf/resources/images/reuters/reuters-logo.svg',
            'bloomberg': 'https://assets.bwbx.io/s3/javelin/public/javelin/images/favicon-black-63fe5249d3.png',
            'cnbc': 'https://www.cnbc.com/a/img/redesign/cnbc_logo.svg',
            'cnn_business': 'https://cdn.cnn.com/cnn/.e/img/3.0/global/misc/cnn-logo.png',
            
            # å­¦è¡“ãƒ»ç ”ç©¶ç³»
            'mit_tech_review': 'https://www.technologyreview.com/favicon.ico',
            'nature': 'https://www.nature.com/static/images/favicons/nature/apple-touch-icon.png',
            'science_daily': 'https://www.sciencedaily.com/favicon.ico',
            
            # é–‹ç™ºè€…å‘ã‘
            'dev_to': 'https://dev.to/favicon.ico',
            'hackernews': 'https://news.ycombinator.com/favicon.ico',
            'medium': 'https://miro.medium.com/v2/resize:fill:152:152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png',
            
            # ä¼æ¥­ãƒ–ãƒ­ã‚°
            'openai_blog': 'https://openai.com/favicon.ico',
            'google_ai': 'https://www.google.com/images/branding/googlelogo/1x/googlelogo_color_272x92dp.png',
            'microsoft_ai': 'https://c.s-microsoft.com/favicon.ico',
            'anthropic_blog': 'https://www.anthropic.com/favicon.ico',
            
            # è¿½åŠ ã®æµ·å¤–ã‚µã‚¤ãƒˆ
            'nextbigfuture': 'https://www.nextbigfuture.com/favicon.ico',
            'singularityhub': 'https://singularityhub.com/favicon.ico',
            
            # ãã®ä»–
            'nikkei': 'https://www.nikkei.com/favicon.ico',
            'yahoo': 'https://s.yimg.com/rz/p/yahoo_frontpage_en-US_s_f_p_205x58_frontpage.png',
            'google': 'https://www.google.com/images/branding/googlelogo/1x/googlelogo_color_272x92dp.png',
            'bbc': 'https://static.files.bbci.co.uk/ws/simorgh-assets/public/news/images/metadata/poster-1024x576.png'
        }
        
        if source in site_logos:
            selected_logo = site_logos[source]
            print(f"  ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆãƒ­ã‚´é¸æŠ: {selected_logo}")
            print(f"  ã‚½ãƒ¼ã‚¹: {source}")
            return selected_logo
        
        # 3. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ±ç”¨çš„ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ã‚¤ã‚³ãƒ³
        fallback_images = [
            'https://dummyimage.com/400x200/007CBA/FFFFFF&text=NEWS',
            'https://dummyimage.com/400x200/28A745/FFFFFF&text=TECH+NEWS',
            'https://dummyimage.com/400x200/DC3545/FFFFFF&text=AI+NEWS',
        ]
        
        import hashlib
        hash_value = int(hashlib.md5(article.title.encode()).hexdigest(), 16)
        selected_image = fallback_images[hash_value % len(fallback_images)]
        
        print(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”»åƒé¸æŠ: {selected_image}")
        print(f"  ã‚½ãƒ¼ã‚¹: {source}")
        
        return selected_image
    
    def create_consolidated_blog_post(self, processed_articles: List) -> dict:
        """çµ±åˆãƒ–ãƒ­ã‚°æŠ•ç¨¿ã‚’ä½œæˆï¼ˆæ”¹è‰¯ç‰ˆï¼šãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»ç¿»è¨³å¯¾å¿œï¼‰"""
        today = datetime.now()
        
        # ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ
        title = f"ä»Šæ—¥ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ {len(processed_articles)}é¸ - {today.strftime('%Yå¹´%mæœˆ%dæ—¥')}"
        
        # å°å…¥æ–‡
        intro = f"""ã“ã‚“ã«ã¡ã¯ï¼ä»Šæ—¥ã‚‚æœ€æ–°ã®AIæƒ…å ±ã‚’ãŠå±Šã‘ã—ã¾ã™ï¼

æœ¬æ—¥ã¯{len(processed_articles)}ä»¶ã®æ³¨ç›®ã™ã¹ãAIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸã€‚ãã‚Œãã‚Œã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ã¤ã„ã¦ã€è¦ç´„ã¨ç§ãŸã¡ã¸ã®å½±éŸ¿ã‚’åˆ†æã—ã¦ãŠä¼ãˆã—ã¾ã™ã€‚

"""
        
        # å„è¨˜äº‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
        content_sections = []
        for i, processed in enumerate(processed_articles, 1):
            article = processed.original_article
            
            # ç¿»è¨³ã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚‹å ´åˆã¯ä½¿ç”¨
            display_title = getattr(processed, 'translated_title', '') or article.title
            
            # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
            preview_text = getattr(processed, 'content_preview', '') or article.content[:200] + "..."
            
            # ç”»åƒURLå–å¾—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            img_url = self.extract_image_from_article(article)
            print(f"è¨˜äº‹{i}ã®ç”»åƒURL: {img_url}")
            
            # ç”»åƒHTMLã‚’æ”¹è‰¯ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆãƒ­ã‚´å¯¾å¿œï¼‰
            source_name = getattr(article, 'source', 'Unknown').title()
            image_html = f'''<div style="text-align: center; margin: 20px 0; padding: 15px; background-color: #f8f9fa; border-radius: 8px; border: 1px solid #e0e0e0;">
<img src="{img_url}" alt="{source_name}ãƒ­ã‚´" style="max-height: 120px; width: auto; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" 
     onerror="
     if (this.src !== 'https://dummyimage.com/400x200/007CBA/FFFFFF&text=NEWS') {{
         this.src='https://dummyimage.com/400x200/007CBA/FFFFFF&text=NEWS';
     }} else {{
         this.style.display='none';
         this.nextElementSibling.innerHTML='ãƒ­ã‚´ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ';
     }}
     ">
<p style="font-size: 12px; color: #666; margin-top: 8px; font-style: italic;">ã‚½ãƒ¼ã‚¹: {source_name}</p>
</div>'''
            
            section = f"""<h2>ğŸ“° {i}. {display_title}</h2>

<blockquote style="background-color: #f8f9fa; border-left: 4px solid #007cba; padding: 15px; margin: 20px 0; border-radius: 5px;">
<p><strong>ã‚½ãƒ¼ã‚¹:</strong> {getattr(article, 'source', 'Unknown')}</p>
<p><strong>å…ƒã‚¿ã‚¤ãƒˆãƒ«:</strong> {article.title if display_title != article.title else 'ï¼ˆä¸Šè¨˜ã¨åŒã˜ï¼‰'}</p>
<p><a href="{article.url}" target="_blank" rel="noopener" style="color: #007cba; text-decoration: none; font-weight: bold;">ğŸ“– å…ƒè¨˜äº‹ã‚’èª­ã‚€ â†’</a></p>
</blockquote>

{image_html}

<div style="background-color: #f0f8ff; padding: 15px; border-radius: 8px; margin: 15px 0;">
<h4>ğŸ” è¨˜äº‹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼</h4>
<p style="font-style: italic; color: #555; line-height: 1.6;">{preview_text}</p>
</div>

<h3>ğŸ“ è¨˜äº‹ã®è¦ç´„</h3>
<p>{processed.summary}</p>

<h3>ğŸ’¡ ç§ãŸã¡ã¸ã®å½±éŸ¿ã¨ä»Šå¾Œã®å±•æœ›</h3>
<p>{processed.user_value_comment}</p>

<hr style="margin: 30px 0; border: none; border-top: 2px solid #e0e0e0;">

"""
            content_sections.append(section)
        
        # ã¾ã¨ã‚æ–‡
        outro = f"""<h2>ğŸ¯ ä»Šæ—¥ã®ã¾ã¨ã‚</h2>

<p>ã„ã‹ãŒã§ã—ãŸã§ã—ã‚‡ã†ã‹ï¼Ÿä»Šæ—¥ã‚‚æ§˜ã€…ãªAIæŠ€è¡“ã®é€²æ­©ãŒè¦‹ã‚‰ã‚Œã¾ã—ãŸã­ï¼</p>

<p>ã“ã‚Œã‚‰ã®æŠ€è¡“å‹•å‘ã¯ã€ç§ãŸã¡ã®æ—¥å¸¸ç”Ÿæ´»ã‚„ä»•äº‹ã«å¤§ããªå¤‰åŒ–ã‚’ã‚‚ãŸã‚‰ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãœã²ã“ã®æƒ…å ±ã‚’å‚è€ƒã«ã€AIæŠ€è¡“ã‚’ç©æ¥µçš„ã«æ´»ç”¨ã—ã¦ã„ã£ã¦ãã ã•ã„ã€‚</p>

<p>ä»–ã«ã‚‚æ°—ã«ãªã‚‹AIæƒ…å ±ãŒã‚ã‚Šã¾ã—ãŸã‚‰ã€ãœã²ã‚³ãƒ¡ãƒ³ãƒˆã§æ•™ãˆã¦ãã ã•ã„ã­ï¼æ˜æ—¥ã‚‚ãŠæ¥½ã—ã¿ã«ï¼</p>
"""
        
        # å…¨ä½“ã‚’çµåˆ
        full_content = intro + ''.join(content_sections) + outro
        
        return {
            'title': title,
            'content': full_content,
            'articles_count': len(processed_articles)
        }
    
    async def run_daily_collection(self):
        """1æ—¥ã®è¨˜äº‹åé›†ãƒ»æŠ•ç¨¿ã‚’å®Ÿè¡Œ"""
        print("=== AIè¨˜äº‹åé›†ãƒ»æŠ•ç¨¿ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹ï¼ˆ1æ—¥1è¨˜äº‹ã¾ã¨ã‚ç‰ˆï¼‰ ===")
        
        # ä»Šæ—¥ã®æŠ•ç¨¿æ•°ã‚’ãƒã‚§ãƒƒã‚¯
        today_count = self.load_today_posts_count()
        
        if today_count >= self.max_posts_per_day:
            print(f"ä»Šæ—¥ã®æŠ•ç¨¿ã¯ã™ã§ã«å®Œäº†ã—ã¦ã„ã¾ã™ï¼ˆ{today_count}/{self.max_posts_per_day}ä»¶ï¼‰ã€‚")
            return
        
        print(f"ä»Šæ—¥ã®æŠ•ç¨¿æ•°: {today_count}/{self.max_posts_per_day}")
        print(f"ä»Šæ—¥ã®ã¾ã¨ã‚è¨˜äº‹ã‚’ä½œæˆã—ã¾ã™ï¼ˆç›®æ¨™: {self.articles_per_post}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰")
        
        try:
            # è¨˜äº‹ã‚’åé›†
            print("\n--- è¨˜äº‹åé›†é–‹å§‹ ---")
            articles = await self.collector.collect_all_sources()
            
            if not articles:
                print("åé›†ã§ããŸè¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                return
            
            print(f"åˆè¨ˆ {len(articles)} ä»¶ã®è¨˜äº‹ã‚’åé›†")
            
            # æœ€é«˜å“è³ªã®è¨˜äº‹ã‚’é¸æŠï¼ˆ5ä»¶ï¼‰
            best_articles = self.select_best_articles(articles, self.articles_per_post)
            print(f"ã¾ã¨ã‚è¨˜äº‹ç”¨ã« {len(best_articles)} ä»¶ã®è¨˜äº‹ã‚’é¸æŠ")
            
            if len(best_articles) < 3:
                print("ååˆ†ãªè¨˜äº‹æ•°ãŒç¢ºä¿ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æœ€ä½3ä»¶å¿…è¦ã§ã™ã€‚")
                return
            
            # å„è¨˜äº‹ã‚’å‡¦ç†ï¼ˆè¦ç´„ãƒ»æ„Ÿæƒ³ç”Ÿæˆï¼‰
            print(f"\n--- {len(best_articles)}ä»¶ã®è¨˜äº‹å‡¦ç†é–‹å§‹ ---")
            processed_articles = []
            
            for i, article in enumerate(best_articles, 1):
                try:
                    print(f"\n[{i}/{len(best_articles)}] å‡¦ç†ä¸­...")
                    print(f"ã‚¿ã‚¤ãƒˆãƒ«: {article.title}")
                    print(f"ã‚½ãƒ¼ã‚¹: {getattr(article, 'source', 'Unknown')}")
                    
                    # è¨˜äº‹ã‚’å‡¦ç†ï¼ˆè¦ç´„ãƒ»æ„Ÿæƒ³ç”Ÿæˆï¼‰
                    processed = self.processor.process_article(article)
                    
                    if processed:
                        processed_articles.append(processed)
                        print("âœ… å‡¦ç†å®Œäº†")
                    else:
                        print("âŒ å‡¦ç†å¤±æ•—")
                        
                except Exception as e:
                    print(f"âŒ è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            if not processed_articles:
                print("å‡¦ç†ã§ããŸè¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                return
            
            print(f"\n=== è¨˜äº‹å‡¦ç†å®Œäº†: {len(processed_articles)}ä»¶æˆåŠŸ ===")
            
            # çµ±åˆãƒ–ãƒ­ã‚°æŠ•ç¨¿ã‚’ä½œæˆ
            print("\n--- çµ±åˆãƒ–ãƒ­ã‚°æŠ•ç¨¿ä½œæˆä¸­ ---")
            blog_post = self.create_consolidated_blog_post(processed_articles)
            
            print(f"çµ±åˆè¨˜äº‹ä½œæˆå®Œäº†:")
            print(f"  ã‚¿ã‚¤ãƒˆãƒ«: {blog_post['title']}")
            print(f"  å«ã¾ã‚Œã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹: {blog_post['articles_count']}ä»¶")
            print(f"  æœ¬æ–‡æ–‡å­—æ•°: {len(blog_post['content'])}æ–‡å­—")
            
            # WordPressã«æŠ•ç¨¿
            print("\n--- WordPressæŠ•ç¨¿ä¸­ ---")
            post_result = self.wp_connector.create_post(
                title=blog_post['title'],
                content=blog_post['content'],
                excerpt=f"æœ¬æ—¥ã®æ³¨ç›®AIãƒ‹ãƒ¥ãƒ¼ã‚¹{blog_post['articles_count']}é¸ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è¦ç´„ã¨å½±éŸ¿åˆ†æã‚’ã”è¦§ãã ã•ã„ã€‚"
            )
            
            if post_result:
                # æŠ•ç¨¿æ•°ã‚’æ›´æ–°
                self.save_today_posts_count(1)
                
                print(f"âœ… æŠ•ç¨¿æˆåŠŸ!")
                print(f"   æŠ•ç¨¿ID: {post_result['id']}")
                print(f"   æŠ•ç¨¿URL: {post_result.get('link', 'N/A')}")
                print(f"   å«ã¾ã‚Œã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹: {blog_post['articles_count']}ä»¶")
                
                print(f"\n=== å‡¦ç†å®Œäº† ===")
                print(f"ä»Šæ—¥ã®ã¾ã¨ã‚è¨˜äº‹æŠ•ç¨¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                
            else:
                print("âŒ WordPressæŠ•ç¨¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
        except Exception as e:
            print(f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
            raise

async def collect_all_news_sources():
    """å…¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã‹ã‚‰è¨˜äº‹ã‚’åé›†ï¼ˆé«˜åº¦ç‰ˆï¼‰"""
    all_articles = []
    
    print("ğŸš€ é«˜åº¦çµ±åˆè¨˜äº‹åé›†é–‹å§‹")
    
    # 1. é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
    try:
        from advanced_scraper import AdvancedScraper
        advanced_scraper = AdvancedScraper(max_concurrent=15)
        
        # å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤
        advanced_scraper.clear_cache(max_age_days=1)
        
        # é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        advanced_articles = await advanced_scraper.collect_all_articles(max_total_articles=20)
        
        # AdvancedArticleã‚’NewsArticleã«å¤‰æ›
        converted_articles = []
        for adv_article in advanced_articles:
            from news_collector import NewsArticle
            article = NewsArticle(
                title=adv_article.title,
                url=adv_article.url,
                content=adv_article.content,
                source=adv_article.source,
                published_date=adv_article.published_date
            )
            converted_articles.append(article)
        
        all_articles.extend(converted_articles)
        print(f"âš¡ é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: {len(converted_articles)}ä»¶")
        
    except Exception as e:
        print(f"âŒ é«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        try:
            collector = AINewsCollector()
            regular_articles = collector.collect_daily_articles()
            all_articles.extend(regular_articles)
            print(f"ğŸ“° ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åé›†: {len(regular_articles)}ä»¶")
        except Exception as e2:
            print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åé›†ã‚¨ãƒ©ãƒ¼: {e2}")
    
    # 2. Xé–¢é€£æƒ…å ±åé›† - å®Œå…¨ç„¡åŠ¹åŒ–
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã«ã‚ˆã‚Šã€Xé¢¨ã®æƒ…å ±ã‚‚å«ã‚ã¦å…¨ã¦ç„¡åŠ¹åŒ–
    print(f"âš ï¸ Xé–¢é€£æƒ…å ±åé›†ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ï¼‰")
    
    # try:
    #     from simple_x_collector import collect_simple_x_posts
    #     x_posts = await collect_simple_x_posts(max_posts=3)
    #     
    #     # XPostã‚’NewsArticleã«å¤‰æ›
    #     x_articles = []
    #     for x_post in x_posts:
    #         from news_collector import NewsArticle
    #         article = NewsArticle(
    #             title=x_post.title,
    #             url=x_post.url,
    #             content=x_post.content,
    #             source=x_post.source,
    #             published_date=x_post.published_date,
    #             author=x_post.author
    #         )
    #         x_articles.append(article)
    #     
    #     all_articles.extend(x_articles)
    #     print(f"ğŸ“± Xé–¢é€£æƒ…å ±: {len(x_articles)}ä»¶")
    #     
    # except Exception as e:
    #     print(f"âŒ Xé–¢é€£æƒ…å ±åé›†ã‚¨ãƒ©ãƒ¼: {e}")
    
    # 3. é‡è¤‡é™¤å»ã¨å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    try:
        filtered_articles = remove_duplicates_and_filter(all_articles)
        print(f"ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ: {len(filtered_articles)}ä»¶")
        
        print(f"ğŸ“Š ç·åˆè¨ˆ: {len(filtered_articles)}ä»¶ã®é«˜å“è³ªè¨˜äº‹ã‚’åé›†")
        return filtered_articles
        
    except Exception as e:
        print(f"âŒ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        return all_articles

def remove_duplicates_and_filter(articles):
    """é‡è¤‡é™¤å»ã¨å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    seen_urls = set()
    seen_titles = set()
    filtered_articles = []
    
    for article in articles:
        # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯
        if article.url in seen_urls:
            continue
        
        # ã‚¿ã‚¤ãƒˆãƒ«é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆé¡ä¼¼åº¦ï¼‰
        title_key = article.title.lower().strip()
        if title_key in seen_titles:
            continue
        
        # å“è³ªãƒã‚§ãƒƒã‚¯
        if len(article.content) < 100:  # æœ€å°æ–‡å­—æ•°
            continue
        
        if len(article.title) < 10:  # æœ€å°ã‚¿ã‚¤ãƒˆãƒ«é•·
            continue
        
        seen_urls.add(article.url)
        seen_titles.add(title_key)
        filtered_articles.append(article)
    
    # æœ€æ–°é †ã«ã‚½ãƒ¼ãƒˆ
    filtered_articles.sort(key=lambda x: x.published_date or "", reverse=True)
    
    return filtered_articles

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        print("=== AIæƒ…å ±åé›†ãƒ»æŠ•ç¨¿ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹ ===")
        
        # çµ±åˆè¨˜äº‹åé›†
        articles = await collect_all_news_sources()
        
        if not articles:
            print("âŒ è¨˜äº‹ãŒåé›†ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        print(f"âœ… {len(articles)}ä»¶ã®è¨˜äº‹ã‚’åé›†ã—ã¾ã—ãŸ")
        
        # è¨˜äº‹å‡¦ç†
        processor = ArticleProcessor()
        processed_articles = processor.process_articles_batch(articles)
        
        if not processed_articles:
            print("âŒ è¨˜äº‹ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        print(f"âœ… {len(processed_articles)}ä»¶ã®è¨˜äº‹ã‚’å‡¦ç†ã—ã¾ã—ãŸ")
        
        # ãƒ–ãƒ­ã‚°æŠ•ç¨¿ç”Ÿæˆ
        blog_generator = BlogPostGenerator()
        blog_post = blog_generator.generate_daily_blog_post(processed_articles)
        
        # WordPressæŠ•ç¨¿
        wp_connector = WordPressConnector()
        if wp_connector.post_article(
            title=blog_post['title'],
            content=blog_post['content'],
            tags=blog_post['tags']
        ):
            print("âœ… WordPressã«æŠ•ç¨¿å®Œäº†")
        else:
            print("âŒ WordPressæŠ•ç¨¿ã«å¤±æ•—")
        
        print("=== ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œå®Œäº† ===")
        
    except Exception as e:
        print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    asyncio.run(main()) 