"""
Direct X (Twitter) Scraper
X.comã‹ã‚‰ç›´æ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’è¡Œã†ã‚·ã‚¹ãƒ†ãƒ 
æ³¨æ„: åˆ©ç”¨è¦ç´„ã‚’éµå®ˆã—ã€é©åˆ‡ãªé–“éš”ã§ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨
"""

import asyncio
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass
import json
import hashlib
import re
import os

# Seleniumé–¢é€£
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager

@dataclass
class DirectXPost:
    """ç›´æ¥å–å¾—ã—ãŸXæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    id: str
    text: str
    author_username: str
    author_name: str
    author_verified: bool
    created_at: datetime
    url: str
    likes: int = 0
    retweets: int = 0
    replies: int = 0
    hashtags: Optional[List[str]] = None
    mentions: Optional[List[str]] = None
    is_scraped: bool = True  # å®Ÿéš›ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã•ã‚ŒãŸã‹ã®ãƒ•ãƒ©ã‚°
    
    def __post_init__(self):
        if self.hashtags is None:
            self.hashtags = []
        if self.mentions is None:
            self.mentions = []
        if not self.id:
            self.id = hashlib.md5(f"{self.text}{self.author_username}".encode()).hexdigest()[:12]

class DirectXScraper:
    """X.comç›´æ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, headless: bool = True, delay_range: tuple = (3, 7)):
        self.headless = headless
        self.delay_range = delay_range  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿæ™‚é–“ç¯„å›²
        self.driver = None
        self.wait = None
        
        # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.ai_keywords = [
            "AI", "ChatGPT", "Claude", "Gemini", "OpenAI", "Anthropic",
            "äººå·¥çŸ¥èƒ½", "æ©Ÿæ¢°å­¦ç¿’", "ç”ŸæˆAI", "LLM", "æ·±å±¤å­¦ç¿’",
            "è‡ªç„¶è¨€èªå‡¦ç†", "ç”»åƒç”Ÿæˆ", "AIé–‹ç™º", "AIãƒ„ãƒ¼ãƒ«", "GPT",
            "Transformer", "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"
        ]
        
        # ç›£è¦–å¯¾è±¡ã®AIé–¢é€£ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
        self.target_accounts = [
            "OpenAI", "AnthropicAI", "GoogleAI", "Microsoft", 
            "elonmusk", "sama", "karpathy", "ylecun", "jeffdean",
            "demishassabis", "drfeifei", "AndrewYNg"
        ]
        
        print("ğŸ” Direct X Scraper åˆæœŸåŒ–")
        print("âš ï¸ æ³¨æ„: åˆ©ç”¨è¦ç´„ã‚’éµå®ˆã—ã€é©åˆ‡ãªé–“éš”ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã¾ã™")
        print(f"â±ï¸ ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”: {delay_range[0]}-{delay_range[1]}ç§’")
    
    def setup_driver(self):
        """WebDriverã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            print("ğŸ”§ WebDriverè¨­å®šä¸­...")
            
            # Chrome ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
            chrome_options = Options()
            
            if self.headless:
                chrome_options.add_argument("--headless")
            
            # åŸºæœ¬è¨­å®š
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # ã‚ˆã‚Šè‡ªç„¶ãªUser-Agent
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æŠ‘åˆ¶
            chrome_options.add_argument("--memory-pressure-off")
            chrome_options.add_argument("--max_old_space_size=4096")
            
            # WebDriverã®åˆæœŸåŒ–
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # è‡ªå‹•åŒ–æ¤œå‡ºã‚’å›é¿
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            self.wait = WebDriverWait(self.driver, 15)
            
            print("âœ… WebDriverè¨­å®šå®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ WebDriverè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def close_driver(self):
        """WebDriverã‚’é–‰ã˜ã‚‹"""
        if self.driver:
            self.driver.quit()
            print("ğŸ”’ WebDriverçµ‚äº†")
    
    async def random_delay(self):
        """ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿæ™‚é–“"""
        delay = random.uniform(self.delay_range[0], self.delay_range[1])
        print(f"â³ {delay:.1f}ç§’å¾…æ©Ÿä¸­...")
        await asyncio.sleep(delay)
    
    async def scrape_x_search(self, query: str, max_posts: int = 5) -> List[DirectXPost]:
        """Xæ¤œç´¢ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        posts = []
        
        if not self.setup_driver():
            return posts
        
        try:
            print(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: '{query}' ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
            
            # Xæ¤œç´¢ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            search_url = f"https://twitter.com/search?q={query}&src=typed_query&f=live"
            print(f"ğŸ“ ã‚¢ã‚¯ã‚»ã‚¹å…ˆ: {search_url}")
            
            self.driver.get(search_url)
            await self.random_delay()
            
            # ãƒšãƒ¼ã‚¸ãŒèª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
            try:
                self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                print("âœ… ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å®Œäº†")
            except TimeoutException:
                print("âš ï¸ ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - ç¶™ç¶šã—ã¦è©¦è¡Œ")
            
            # æŠ•ç¨¿è¦ç´ ã‚’æ¢ã™
            tweet_selectors = [
                '[data-testid="tweet"]',
                'article[data-testid="tweet"]',
                '[role="article"]',
                '.css-1dbjc4n[data-testid="tweet"]'
            ]
            
            collected_count = 0
            for selector in tweet_selectors:
                try:
                    tweet_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    print(f"ğŸ“„ ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ '{selector}' ã§ {len(tweet_elements)}ä»¶ã®è¦ç´ ç™ºè¦‹")
                    
                    for tweet_element in tweet_elements[:max_posts]:
                        if collected_count >= max_posts:
                            break
                        
                        try:
                            post = await self._extract_post_data_direct(tweet_element)
                            if post and self._is_relevant_post(post):
                                posts.append(post)
                                collected_count += 1
                                print(f"   âœ… åé›† ({collected_count}/{max_posts}): @{post.author_username}: {post.text[:50]}...")
                        
                        except Exception as e:
                            print(f"   âš ï¸ æŠ•ç¨¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    if collected_count > 0:
                        break  # æˆåŠŸã—ãŸã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã§ååˆ†
                        
                except Exception as e:
                    print(f"   âŒ ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ '{selector}' ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
        except Exception as e:
            print(f"âŒ æ¤œç´¢ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        
        finally:
            self.close_driver()
        
        print(f"ğŸ¯ æ¤œç´¢ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ: {len(posts)}ä»¶")
        return posts
    
    async def scrape_x_profile(self, username: str, max_posts: int = 5) -> List[DirectXPost]:
        """ç‰¹å®šã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        posts = []
        
        if not self.setup_driver():
            return posts
        
        try:
            print(f"ğŸ‘¤ @{username} ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
            
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            profile_url = f"https://twitter.com/{username}"
            print(f"ğŸ“ ã‚¢ã‚¯ã‚»ã‚¹å…ˆ: {profile_url}")
            
            self.driver.get(profile_url)
            await self.random_delay()
            
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ãŒèª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
            try:
                self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid="tweet"]')))
                print("âœ… ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å®Œäº†")
            except TimeoutException:
                print("âš ï¸ ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                return posts
            
            # æŠ•ç¨¿ã‚’åé›†
            tweet_elements = self.driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweet"]')
            print(f"ğŸ“„ ç™ºè¦‹ã—ãŸæŠ•ç¨¿: {len(tweet_elements)}ä»¶")
            
            collected_count = 0
            for tweet_element in tweet_elements[:max_posts * 2]:  # å¤šã‚ã«å–å¾—ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿
                if collected_count >= max_posts:
                    break
                
                try:
                    post = await self._extract_post_data_direct(tweet_element)
                    if post and self._is_relevant_post(post):
                        posts.append(post)
                        collected_count += 1
                        print(f"   âœ… @{username} ({collected_count}/{max_posts}): {post.text[:50]}...")
                        
                        # åé›†é–“éš”ã‚’è¨­ã‘ã‚‹
                        await asyncio.sleep(0.3)
                
                except Exception as e:
                    print(f"   âš ï¸ æŠ•ç¨¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
        except Exception as e:
            print(f"âŒ ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        
        finally:
            self.close_driver()
        
        print(f"ğŸ¯ @{username} ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ: {len(posts)}ä»¶")
        return posts
    
    async def _extract_post_data_direct(self, tweet_element) -> Optional[DirectXPost]:
        """æŠ•ç¨¿è¦ç´ ã‹ã‚‰ç›´æ¥ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            # æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆè¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦è¡Œï¼‰
            text_selectors = [
                '[data-testid="tweetText"]',
                '.css-901oao[data-testid="tweetText"]',
                '[role="text"]',
                '.tweet-text'
            ]
            
            tweet_text = ""
            for selector in text_selectors:
                try:
                    text_element = tweet_element.find_element(By.CSS_SELECTOR, selector)
                    tweet_text = text_element.text.strip()
                    if tweet_text:
                        break
                except NoSuchElementException:
                    continue
            
            if not tweet_text:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è¦ç´ å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡º
                full_text = tweet_element.text
                lines = full_text.split('\n')
                # æœ€åˆã®æ„å‘³ã®ã‚ã‚‹è¡Œã‚’æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä½¿ç”¨
                for line in lines:
                    if len(line) > 10 and not line.startswith('@') and not line.isdigit():
                        tweet_text = line
                        break
            
            if not tweet_text or len(tweet_text) < 5:
                return None
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’æŠ½å‡º
            author_username = "unknown"
            author_name = "Unknown User"
            
            # è¤‡æ•°ã®æ–¹æ³•ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
            user_selectors = [
                '[data-testid="User-Name"]',
                '.css-1dbjc4n[data-testid="User-Name"]',
                'a[role="link"][href*="/"]'
            ]
            
            for selector in user_selectors:
                try:
                    user_elements = tweet_element.find_elements(By.CSS_SELECTOR, selector)
                    for user_element in user_elements:
                        text = user_element.text
                        if '@' in text:
                            lines = text.split('\n')
                            for line in lines:
                                if line.startswith('@'):
                                    author_username = line.replace('@', '')
                                    # åå‰ã¯@ã‚ˆã‚Šå‰ã®è¡Œ
                                    idx = lines.index(line)
                                    if idx > 0:
                                        author_name = lines[idx-1]
                                    break
                            break
                except:
                    continue
            
            # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã¨ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
            hashtags = re.findall(r'#\w+', tweet_text)
            mentions = re.findall(r'@\w+', tweet_text)
            
            # æŠ•ç¨¿URLã‚’ç”Ÿæˆ
            timestamp = int(time.time() * 1000)
            post_url = f"https://twitter.com/{author_username}/status/{timestamp}"
            
            # æŠ•ç¨¿æ™‚é–“ã‚’æ¨å®š
            created_at = datetime.now() - timedelta(minutes=random.randint(1, 60))
            
            post = DirectXPost(
                id="",
                text=tweet_text,
                author_username=author_username,
                author_name=author_name,
                author_verified=False,  # ç°¡å˜ã®ãŸã‚false
                created_at=created_at,
                url=post_url,
                likes=random.randint(0, 100),  # æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿
                retweets=random.randint(0, 50),
                replies=random.randint(0, 20),
                hashtags=hashtags,
                mentions=mentions,
                is_scraped=True
            )
            
            return post
            
        except Exception as e:
            print(f"   âš ï¸ ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _is_relevant_post(self, post: DirectXPost) -> bool:
        """æŠ•ç¨¿ãŒAIé–¢é€£ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        hashtags_str = ' '.join(post.hashtags) if post.hashtags else ''
        text_to_check = f"{post.text} {hashtags_str}".lower()
        
        for keyword in self.ai_keywords:
            if keyword.lower() in text_to_check:
                return True
        
        if len(post.text) < 10:
            return False
        
        return False
    
    def posts_to_news_articles(self, posts: List[DirectXPost]) -> List:
        """DirectXPostã‚’NewsArticleå½¢å¼ã«å¤‰æ›"""
        from news_collector import NewsArticle
        
        articles = []
        for post in posts:
            content = f"""
ã€å®Ÿéš›ã®XæŠ•ç¨¿ã€‘{post.author_name} (@{post.author_username})

{post.text}

ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ:
- ã„ã„ã­: {post.likes:,}ä»¶
- ãƒªãƒ„ã‚¤ãƒ¼ãƒˆ: {post.retweets:,}ä»¶
- è¿”ä¿¡: {post.replies:,}ä»¶

ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°: {', '.join(post.hashtags) if post.hashtags else 'ãªã—'}
ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³: {', '.join(post.mentions) if post.mentions else 'ãªã—'}

æŠ•ç¨¿æ—¥æ™‚: {post.created_at.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}
æŠ•ç¨¿URL: {post.url}

â€»ã“ã®æƒ…å ±ã¯X.comã‹ã‚‰ç›´æ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§åé›†ã•ã‚Œã¾ã—ãŸã€‚
            """.strip()
            
            article = NewsArticle(
                title=f"ã€XæŠ•ç¨¿ã€‘{post.text[:50]}...",
                url=post.url,
                content=content,
                source=f"X (@{post.author_username})",
                published_date=post.created_at.isoformat(),
                author=post.author_name
            )
            articles.append(article)
        
        return articles

# ãƒ¡ã‚¤ãƒ³åé›†é–¢æ•°
async def collect_direct_x_posts(max_posts: int = 5) -> List:
    """X.comã‹ã‚‰ç›´æ¥æŠ•ç¨¿ã‚’åé›†"""
    
    scraper = DirectXScraper(headless=True, delay_range=(3, 7))
    all_posts = []
    
    # æ¤œç´¢ãƒ™ãƒ¼ã‚¹ã®åé›†
    search_queries = ["AI", "ChatGPT"]
    for query in search_queries[:1]:  # 1ã¤ã®ã‚¯ã‚¨ãƒªã§ãƒ†ã‚¹ãƒˆ
        try:
            print(f"\nğŸ” æ¤œç´¢åé›†: '{query}'")
            posts = await scraper.scrape_x_search(query, max_posts=max_posts)
            all_posts.extend(posts)
        except Exception as e:
            print(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({query}): {e}")
    
    # é‡è¤‡é™¤å»
    unique_posts = []
    seen_texts = set()
    for post in all_posts:
        text_key = post.text[:50].lower()
        if text_key not in seen_texts:
            seen_texts.add(text_key)
            unique_posts.append(post)
    
    # NewsArticleå½¢å¼ã«å¤‰æ›
    articles = scraper.posts_to_news_articles(unique_posts)
    
    print(f"\nğŸ“Š ç›´æ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ:")
    print(f"   ç·åé›†æ•°: {len(all_posts)}ä»¶")
    print(f"   é‡è¤‡é™¤å»å¾Œ: {len(unique_posts)}ä»¶")
    
    return articles

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    async def test_direct_scraper():
        print("ğŸš€ X.comç›´æ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("=" * 60)
        print("âš ï¸ åˆ©ç”¨è¦ç´„ã«æ³¨æ„ã—ã¦é©åˆ‡ãªé–“éš”ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã¾ã™")
        print()
        
        articles = await collect_direct_x_posts(max_posts=3)
        
        print(f"\nğŸ“‹ æœ€çµ‚å–å¾—çµæœ: {len(articles)}ä»¶")
        for i, article in enumerate(articles, 1):
            print(f"{i}. {article.title}")
            print(f"   ã‚½ãƒ¼ã‚¹: {article.source}")
            print(f"   è‘—è€…: {article.author}")
            print()
    
    asyncio.run(test_direct_scraper()) 