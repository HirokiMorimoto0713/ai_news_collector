"""
Advanced X (Twitter) Post Scraper with Selenium
Seleniumã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªXæŠ•ç¨¿åé›†ã‚·ã‚¹ãƒ†ãƒ 
æ³¨æ„: åˆ©ç”¨è¦ç´„ã‚’éµå®ˆã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨
"""

import asyncio
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass
import json
import hashlib
import re

# Seleniumé–¢é€£
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager

@dataclass
class AdvancedXPost:
    """é«˜åº¦ãªXæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    id: str
    text: str
    author_username: str
    author_name: str
    author_verified: bool
    created_at: datetime
    url: str
    likes: int = 0
    retweets: int = 0
    replies: int = 0
    hashtags: List[str] = None
    mentions: List[str] = None
    
    def __post_init__(self):
        if self.hashtags is None:
            self.hashtags = []
        if self.mentions is None:
            self.mentions = []
        if not self.id:
            self.id = hashlib.md5(f"{self.text}{self.author_username}".encode()).hexdigest()[:12]

class AdvancedXScraper:
    """é«˜åº¦ãªXæŠ•ç¨¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, headless: bool = True):
        self.headless = headless
        self.driver = None
        self.wait = None
        
        self.ai_keywords = [
            "AI", "ChatGPT", "Claude", "Gemini", "OpenAI", "Anthropic",
            "äººå·¥çŸ¥èƒ½", "æ©Ÿæ¢°å­¦ç¿’", "ç”ŸæˆAI", "LLM", "æ·±å±¤å­¦ç¿’",
            "è‡ªç„¶è¨€èªå‡¦ç†", "ç”»åƒç”Ÿæˆ", "AIé–‹ç™º", "AIãƒ„ãƒ¼ãƒ«"
        ]
        
        # æœ‰åãªAIé–¢é€£ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
        self.ai_accounts = [
            "OpenAI", "AnthropicAI", "GoogleAI", "Microsoft", 
            "elonmusk", "sama", "karpathy", "ylecun"
        ]
        
        print("ğŸ¤– Advanced X Scraper åˆæœŸåŒ–")
        print("âš ï¸ æ³¨æ„: åˆ©ç”¨è¦ç´„ã‚’éµå®ˆã—ã¦ä½¿ç”¨ã—ã¦ãã ã•ã„")
    
    def setup_driver(self):
        """WebDriverã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            print("ğŸ”§ WebDriverè¨­å®šä¸­...")
            
            # Chrome ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
            chrome_options = Options()
            
            if self.headless:
                chrome_options.add_argument("--headless")
            
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            
            # ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼è¨­å®š
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # WebDriverã®åˆæœŸåŒ–
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # è‡ªå‹•åŒ–æ¤œå‡ºã‚’å›é¿
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            self.wait = WebDriverWait(self.driver, 10)
            
            print("âœ… WebDriverè¨­å®šå®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ WebDriverè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def close_driver(self):
        """WebDriverã‚’é–‰ã˜ã‚‹"""
        if self.driver:
            self.driver.quit()
            print("ğŸ”’ WebDriverçµ‚äº†")
    
    async def scrape_x_posts_by_search(self, query: str, max_posts: int = 5) -> List[AdvancedXPost]:
        """æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§XæŠ•ç¨¿ã‚’åé›†"""
        posts = []
        
        if not self.setup_driver():
            return posts
        
        try:
            print(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: '{query}' ã§æŠ•ç¨¿åé›†ä¸­...")
            
            # Xæ¤œç´¢ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            search_url = f"https://twitter.com/search?q={query}&src=typed_query&f=live"
            self.driver.get(search_url)
            
            # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ
            await asyncio.sleep(3)
            
            # æŠ•ç¨¿è¦ç´ ã‚’åé›†
            collected_count = 0
            scroll_attempts = 0
            max_scroll_attempts = 3
            
            while collected_count < max_posts and scroll_attempts < max_scroll_attempts:
                try:
                    # æŠ•ç¨¿è¦ç´ ã‚’å–å¾—
                    tweet_elements = self.driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweet"]')
                    
                    for tweet_element in tweet_elements[collected_count:]:
                        if collected_count >= max_posts:
                            break
                        
                        try:
                            post = await self._extract_post_data(tweet_element)
                            if post and self._is_relevant_post(post):
                                posts.append(post)
                                collected_count += 1
                                print(f"   âœ… åé›† ({collected_count}/{max_posts}): {post.text[:50]}...")
                        
                        except Exception as e:
                            print(f"   âš ï¸ æŠ•ç¨¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    # æ–°ã—ã„æŠ•ç¨¿ã‚’èª­ã¿è¾¼ã‚€ãŸã‚ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                    if collected_count < max_posts:
                        self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                        await asyncio.sleep(2)
                        scroll_attempts += 1
                    
                except Exception as e:
                    print(f"   âŒ ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    break
            
        except Exception as e:
            print(f"âŒ æ¤œç´¢åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        finally:
            self.close_driver()
        
        print(f"ğŸ¯ æ¤œç´¢åé›†çµæœ: {len(posts)}ä»¶")
        return posts
    
    async def scrape_x_posts_by_accounts(self, accounts: List[str], max_posts_per_account: int = 2) -> List[AdvancedXPost]:
        """ç‰¹å®šã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰XæŠ•ç¨¿ã‚’åé›†"""
        all_posts = []
        
        if not self.setup_driver():
            return all_posts
        
        try:
            for account in accounts[:3]:  # æœ€å¤§3ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
                try:
                    print(f"ğŸ‘¤ @{account} ã®æŠ•ç¨¿ã‚’åé›†ä¸­...")
                    
                    # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
                    profile_url = f"https://twitter.com/{account}"
                    self.driver.get(profile_url)
                    
                    # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ
                    await asyncio.sleep(3)
                    
                    # æŠ•ç¨¿è¦ç´ ã‚’å–å¾—
                    tweet_elements = self.driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweet"]')
                    
                    account_posts = []
                    for tweet_element in tweet_elements[:max_posts_per_account * 2]:  # å¤šã‚ã«å–å¾—ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿
                        try:
                            post = await self._extract_post_data(tweet_element)
                            if post and self._is_relevant_post(post) and len(account_posts) < max_posts_per_account:
                                account_posts.append(post)
                                print(f"   âœ… @{account}: {post.text[:50]}...")
                        
                        except Exception as e:
                            print(f"   âš ï¸ æŠ•ç¨¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    all_posts.extend(account_posts)
                    
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                    await asyncio.sleep(random.uniform(2, 5))
                
                except Exception as e:
                    print(f"   âŒ @{account} åé›†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        except Exception as e:
            print(f"âŒ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        finally:
            self.close_driver()
        
        print(f"ğŸ¯ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåé›†çµæœ: {len(all_posts)}ä»¶")
        return all_posts
    
    async def _extract_post_data(self, tweet_element) -> Optional[AdvancedXPost]:
        """æŠ•ç¨¿è¦ç´ ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            # æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            try:
                text_element = tweet_element.find_element(By.CSS_SELECTOR, '[data-testid="tweetText"]')
                tweet_text = text_element.text
            except NoSuchElementException:
                return None
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
            try:
                user_element = tweet_element.find_element(By.CSS_SELECTOR, '[data-testid="User-Name"]')
                user_info = user_element.text.split('\n')
                author_name = user_info[0] if user_info else "Unknown"
                author_username = user_info[1].replace('@', '') if len(user_info) > 1 else "unknown"
            except NoSuchElementException:
                author_name = "Unknown"
                author_username = "unknown"
            
            # èªè¨¼ãƒãƒƒã‚¸ã‚’ãƒã‚§ãƒƒã‚¯
            try:
                verified_element = tweet_element.find_element(By.CSS_SELECTOR, '[data-testid="icon-verified"]')
                author_verified = True
            except NoSuchElementException:
                author_verified = False
            
            # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—
            likes = self._extract_engagement_count(tweet_element, 'like')
            retweets = self._extract_engagement_count(tweet_element, 'retweet')
            replies = self._extract_engagement_count(tweet_element, 'reply')
            
            # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã¨ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
            hashtags = re.findall(r'#\w+', tweet_text)
            mentions = re.findall(r'@\w+', tweet_text)
            
            # æŠ•ç¨¿URLã‚’ç”Ÿæˆ
            post_url = f"https://twitter.com/{author_username}/status/{int(time.time() * 1000)}"
            
            post = AdvancedXPost(
                id="",
                text=tweet_text,
                author_username=author_username,
                author_name=author_name,
                author_verified=author_verified,
                created_at=datetime.now(),
                url=post_url,
                likes=likes,
                retweets=retweets,
                replies=replies,
                hashtags=hashtags,
                mentions=mentions
            )
            
            return post
            
        except Exception as e:
            print(f"   âš ï¸ ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _extract_engagement_count(self, tweet_element, engagement_type: str) -> int:
        """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ•°ã‚’æŠ½å‡º"""
        try:
            engagement_element = tweet_element.find_element(By.CSS_SELECTOR, f'[data-testid="{engagement_type}"]')
            count_text = engagement_element.text
            
            if not count_text or count_text.isspace():
                return 0
            
            # æ•°å€¤ã‚’æŠ½å‡ºï¼ˆK, M ãªã©ã®å˜ä½ã‚‚å‡¦ç†ï¼‰
            count_text = count_text.replace(',', '')
            if 'K' in count_text:
                return int(float(count_text.replace('K', '')) * 1000)
            elif 'M' in count_text:
                return int(float(count_text.replace('M', '')) * 1000000)
            else:
                return int(re.sub(r'[^\d]', '', count_text)) if count_text.isdigit() else 0
        
        except (NoSuchElementException, ValueError):
            return 0
    
    def _is_relevant_post(self, post: AdvancedXPost) -> bool:
        """æŠ•ç¨¿ãŒAIé–¢é€£ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        text_to_check = f"{post.text} {' '.join(post.hashtags)}".lower()
        
        # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
        for keyword in self.ai_keywords:
            if keyword.lower() in text_to_check:
                return True
        
        # èªè¨¼æ¸ˆã¿ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¯å„ªå…ˆ
        if post.author_verified and any(ai_account.lower() in post.author_username.lower() for ai_account in self.ai_accounts):
            return True
        
        return False
    
    def posts_to_news_articles(self, posts: List[AdvancedXPost]) -> List:
        """AdvancedXPostã‚’NewsArticleå½¢å¼ã«å¤‰æ›"""
        from news_collector import NewsArticle
        
        articles = []
        for post in posts:
            content = f"""
ã€å®Ÿéš›ã®XæŠ•ç¨¿ã€‘{post.author_name} (@{post.author_username})
{' âœ“ èªè¨¼æ¸ˆã¿' if post.author_verified else ''}

{post.text}

ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ:
- ã„ã„ã­: {post.likes:,}ä»¶
- ãƒªãƒ„ã‚¤ãƒ¼ãƒˆ: {post.retweets:,}ä»¶
- è¿”ä¿¡: {post.replies:,}ä»¶

ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°: {', '.join(post.hashtags) if post.hashtags else 'ãªã—'}
ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³: {', '.join(post.mentions) if post.mentions else 'ãªã—'}

æŠ•ç¨¿æ—¥æ™‚: {post.created_at.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}
æŠ•ç¨¿URL: {post.url}

â€»ã“ã®æƒ…å ±ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹XæŠ•ç¨¿ã‹ã‚‰åé›†ã•ã‚ŒãŸã‚‚ã®ã§ã™ã€‚
            """.strip()
            
            article = NewsArticle(
                title=f"ã€XæŠ•ç¨¿ã€‘{post.text[:50]}...",
                url=post.url,
                content=content,
                source=f"X (@{post.author_username})",
                published_date=post.created_at.isoformat(),
                author=post.author_name
            )
            articles.append(article)
        
        return articles

# ãƒ¡ã‚¤ãƒ³åé›†é–¢æ•°
async def collect_advanced_x_posts(search_queries: List[str] = None, max_posts: int = 5) -> List:
    """
    é«˜åº¦ãªXæŠ•ç¨¿åé›†
    
    Args:
        search_queries: æ¤œç´¢ã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆ
        max_posts: æœ€å¤§æŠ•ç¨¿æ•°
    
    Returns:
        NewsArticleå½¢å¼ã®è¨˜äº‹ãƒªã‚¹ãƒˆ
    """
    if search_queries is None:
        search_queries = ["AI", "ChatGPT", "äººå·¥çŸ¥èƒ½"]
    
    scraper = AdvancedXScraper(headless=True)
    all_posts = []
    
    # æ¤œç´¢ãƒ™ãƒ¼ã‚¹ã®åé›†
    for query in search_queries[:2]:  # æœ€å¤§2ã¤ã®ã‚¯ã‚¨ãƒª
        try:
            posts = await scraper.scrape_x_posts_by_search(query, max_posts=3)
            all_posts.extend(posts)
        except Exception as e:
            print(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({query}): {e}")
    
    # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã®åé›†
    try:
        account_posts = await scraper.scrape_x_posts_by_accounts(
            ["OpenAI", "AnthropicAI", "GoogleAI"], 
            max_posts_per_account=2
        )
        all_posts.extend(account_posts)
    except Exception as e:
        print(f"âŒ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåé›†ã‚¨ãƒ©ãƒ¼: {e}")
    
    # é‡è¤‡é™¤å»
    unique_posts = []
    seen_texts = set()
    for post in all_posts:
        text_key = post.text[:50].lower()
        if text_key not in seen_texts:
            seen_texts.add(text_key)
            unique_posts.append(post)
    
    # æœ€æ–°é †ã§ã‚½ãƒ¼ãƒˆ
    sorted_posts = sorted(unique_posts, key=lambda p: p.created_at, reverse=True)
    final_posts = sorted_posts[:max_posts]
    
    # NewsArticleå½¢å¼ã«å¤‰æ›
    articles = scraper.posts_to_news_articles(final_posts)
    
    print(f"\nğŸ“Š é«˜åº¦ãªXæŠ•ç¨¿åé›†çµæœ:")
    print(f"   ç·åé›†æ•°: {len(all_posts)}ä»¶")
    print(f"   é‡è¤‡é™¤å»å¾Œ: {len(unique_posts)}ä»¶")
    print(f"   æœ€çµ‚é¸æŠ: {len(final_posts)}ä»¶")
    
    return articles

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    async def test_advanced_scraper():
        print("ğŸš€ é«˜åº¦ãªXæŠ•ç¨¿åé›†ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("=" * 60)
        
        # åŸºæœ¬ãƒ†ã‚¹ãƒˆ
        articles = await collect_advanced_x_posts(
            search_queries=["AI", "ChatGPT"], 
            max_posts=5
        )
        
        print(f"\nğŸ“‹ å–å¾—çµæœ: {len(articles)}ä»¶")
        for i, article in enumerate(articles, 1):
            print(f"{i}. {article.title}")
            print(f"   ã‚½ãƒ¼ã‚¹: {article.source}")
            print(f"   è‘—è€…: {article.author}")
            print()
    
    asyncio.run(test_advanced_scraper()) 